{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import boto3\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "from decimal import Decimal\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "#from torch.nn.functional import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploading the environment variables and get the API key\n",
    "load_dotenv()\n",
    "HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "AWS_ACCESS_KEY = os.getenv(\"AWS_ACCESS_KEY\")\n",
    "AWS_SECRET_KEY = os.getenv(\"AWS_SECRET_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Data from FireBase (remains missing the conextion to firebase collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\KMSpico\\temp\\ipykernel_9272\\1355746993.py:2: DtypeWarning: Columns (0) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  filtered_data = pd.read_csv(r'../../../content_data_from_firebase.csv',sep=',')\n"
     ]
    }
   ],
   "source": [
    "# From FireBase\n",
    "filtered_data = pd.read_csv(r'../../../content_data_from_firebase.csv',sep=',')\n",
    "filtered_data = pd.DataFrame(filtered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 254998 entries, 0 to 264264\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   ID            254998 non-null  object\n",
      " 1   Genre         254998 non-null  object\n",
      " 2   CleanTitle    254998 non-null  object\n",
      " 3   Synopsis      254998 non-null  object\n",
      " 4   Directors     254998 non-null  object\n",
      " 5   Cast          254998 non-null  object\n",
      " 6   Type          254998 non-null  object\n",
      " 7   PlatformName  254998 non-null  object\n",
      "dtypes: object(8)\n",
      "memory usage: 17.5+ MB\n"
     ]
    }
   ],
   "source": [
    "filtered_data = filtered_data.replace(\"\",pd.NA)\n",
    "filtered_data = filtered_data.dropna()\n",
    "filtered_data = filtered_data.drop_duplicates()\n",
    "filtered_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Preferences from DynamoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the info from DynamoDB, user preferences\n",
    "CONFIG = {\n",
    "    'aws': {\n",
    "        'access_key': AWS_ACCESS_KEY,\n",
    "        'secret_key': AWS_SECRET_KEY,\n",
    "        'region': 'eu-west-3',\n",
    "        'table': 'User-7kkcm5dn2rb77hst5nh7gbdisa-staging'\n",
    "    },\n",
    "    'columns': ['userId', 'favoriteMoviesIds', 'favoriteGenresIds', 'favoriteSeriesIds'],\n",
    "}\n",
    "\n",
    "# conexion\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=CONFIG['aws']['access_key'],\n",
    "    aws_secret_access_key=CONFIG['aws']['secret_key'],\n",
    "    region_name=CONFIG['aws']['region']\n",
    ")\n",
    "\n",
    "table = session.resource('dynamodb').Table(CONFIG['aws']['table'])\n",
    "\n",
    "# Values to String\n",
    "def _process_value(value):\n",
    "    if isinstance(value, Decimal):\n",
    "        return str(int(value))\n",
    "    return str(value)\n",
    "\n",
    "# Retrive info from DynamoDB and gets a DataFrame\n",
    "def fetch_preferences():\n",
    "    try:\n",
    "        items = []\n",
    "        start_key = None\n",
    "\n",
    "        while True:\n",
    "            # scan with defined 'columns'  in previous 'CONFIG'\n",
    "            scan_params = {\n",
    "                'ProjectionExpression': ', '.join(CONFIG['columns'])\n",
    "            }\n",
    "            if start_key:\n",
    "                scan_params['ExclusiveStartKey'] = start_key\n",
    "\n",
    "            response = table.scan(**scan_params)\n",
    "            items.extend(response.get('Items', []))\n",
    "\n",
    "            # check for next pages\n",
    "            start_key = response.get('LastEvaluatedKey')\n",
    "            if not start_key:\n",
    "                break\n",
    "\n",
    "        # data extracted processing\n",
    "        processed_data = [{\n",
    "            'userId': _process_value(item.get('userId', '')),\n",
    "            'favoriteMoviesIds': ';'.join(map(_process_value, item.get('favoriteMoviesIds', []))),\n",
    "            'favoriteGenresIds': ';'.join(map(_process_value, item.get('favoriteGenresIds', []))),\n",
    "            'favoriteSeriesIds': ';'.join(map(_process_value, item.get('favoriteSeriesIds', [])))\n",
    "        } for item in items]\n",
    "\n",
    "        df = pd.DataFrame(processed_data)\n",
    "        return df\n",
    "\n",
    "    except ClientError as e:\n",
    "        print(f\"Error al conectar con DynamoDB: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# calling function to get the df\n",
    "user_pref = fetch_preferences()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limpio el dataframe dejando solo users con genero, movie_favs y tvshow_favs\n",
    "user_pref = user_pref[user_pref['userId'].str.len()==36]\n",
    "user_pref = user_pref.replace(\"\",pd.NA)\n",
    "user_pref = user_pref.dropna()\n",
    "user_pref.reset_index(inplace=True,drop=True)\n",
    "print(f'Duplicates: {user_pref.duplicated().sum()}')\n",
    "user_pref.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# celda para producir no terminada\n",
    "\n",
    "\n",
    "# convertir los valores en listas para expandirlos con explode\n",
    "###user_pref['favoriteMoviesIds'] = user_pref['favoriteMoviesIds'].apply(lambda x: x.split(';'))\n",
    "\n",
    "# expandir preferencias de favoriteMoviesIds, favoriteGenresIds, y favoriteSeriesIds por userId\n",
    "###user_fav_movies = user_pref[['userId','favoriteMoviesIds']].explode('favoriteMoviesIds')\n",
    "\n",
    "\n",
    "# merge para traerme los CleanTitle, Synopsis, 'Genre'\n",
    "###user_fav_movies = user_fav_movies.merge(filtered_data[['ID','CleanTitle']], left_on='favoriteMoviesIds', right_on='ID')\n",
    "###user_fav_movies = user_fav_movies.drop(columns='ID')\n",
    "\n",
    "# reAGRUPO por userId para que me queden las listas CleanTitle, Synopsis, 'Genre' por user segun sus favoriteMoviesIds, favoriteGenresIds, y favoriteSeriesIds por userId\n",
    "user_pref_prueba = user_fav_movies.groupby('userId')[['favoriteMoviesIds','CleanTitle']].agg(list).reset_index()\n",
    "\n",
    "\n",
    "user_pref_prueba\n",
    "\n",
    "### >>>>>>>>>>>> TERMINAR DE HACER LO MISMO CON GENERO Y SERIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celda para borrar\n",
    "user_pref.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#celda para borrar\n",
    "filtered_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences for vectorize from 'filtered_data' / AHORA DESDE FIREBASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the sentences to embed \n",
    "filtered_data['sentences_to_embed'] = (\n",
    "    filtered_data.CleanTitle.fillna('') +\n",
    "    filtered_data.Synopsis.fillna('') +\n",
    "    filtered_data.Genre.fillna('').apply(\n",
    "        lambda x: ', '.join(ast.literal_eval(x)) if x.startswith('[') and x.endswith(']') else x\n",
    "    )\n",
    ")\n",
    "\n",
    "# Formatting as str-list to send to the model\n",
    "sentences_from_filtered_data = filtered_data.sentences_to_embed.dropna().astype(str).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the model on Hugging Face processes only requests that can be completed within 60 seconds, we need to divide the sentences into batches.\n",
    "def split_into_batches(sentences, batch_size):\n",
    "    return [sentences[i:i + batch_size] for i in range(0, len(sentences), batch_size)]\n",
    "\n",
    "# After trying with different values, we've reach the maximum batch size to get response succesfully\n",
    "batches = split_into_batches(sentences_from_filtered_data, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check key availability\n",
    "if HUGGINGFACE_API_KEY is None:\n",
    "    print(\"Error: No se encontró la clave de API de Hugging Face.\")\n",
    "else:\n",
    "    print(\"Clave de API cargada correctamente.\")\n",
    "\n",
    "# Model URL\n",
    "API_URL = \"https://api-inference.huggingface.co/pipeline/feature-extraction/sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# API header and key\n",
    "headers = {\"Authorization\": f\"Bearer {HUGGINGFACE_API_KEY}\"}  \n",
    "\n",
    "# Function to get embeddings from Hugging Face API\n",
    "def get_embeddings_from_api(sentences):\n",
    "    url = API_URL\n",
    "    payload = {\"inputs\": sentences}\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error {response.status_code}: {response.text}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "# Sequence Batch Processing Process\n",
    "all_embeddings_from_filtered_data = []\n",
    "for batch in batches:\n",
    "    print(f\"Processing batch with {len(batch)} sentences...\")\n",
    "    embeddings = get_embeddings_from_api(batch)\n",
    "    if embeddings:\n",
    "        all_embeddings_from_filtered_data.extend(embeddings)\n",
    "\n",
    "print(\"Embeddings processed successfully:\")\n",
    "print(all_embeddings_from_filtered_data[:2])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esta sección es para agregar los titulos de los generos, y los str correspondientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences for vectorize from 'user_preferences' / AHORA DESDE DYNAMODB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentences we want to be embedded from user_preferences\n",
    "user_pref['sentences_to_embed'] = (user_pref.TitulosPeliculas.fillna('') +\n",
    "                                   user_pref.TitulosSeries.fillna('') + \n",
    "                                   user_pref.GenerosFavoritos.fillna('') +\n",
    "                                   user_pref.DetallesPeliculas.fillna('') +\n",
    "                                   user_pref.DetallesSeries.fillna(''))\n",
    "\n",
    "# Formatting as str-list to send to the model\n",
    "sentences_from_user_pref = user_pref.sentences_to_embed.dropna().astype(str).tolist()\n",
    "\n",
    "# We split the sentences in batches as we did previously with filtered_data\n",
    "batches_user_pref = split_into_batches(sentences_from_user_pref, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence Batch Processing Process\n",
    "all_embeddings_from_user_pref = []\n",
    "for batch in batches_user_pref:\n",
    "    print(f\"Processing batch with {len(batch)} sentences...\")\n",
    "    embeddings = get_embeddings_from_api(batch)\n",
    "    if embeddings:\n",
    "        all_embeddings_from_user_pref.extend(embeddings)\n",
    "\n",
    "print(\"Embeddings processed successfully:\")\n",
    "print(all_embeddings_from_user_pref[:2])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to convert the embeddings list to numpy arrays in order to calculate cosine similarities wiht sklearn\n",
    "all_embeddings_from_user_pref_array = np.array(all_embeddings_from_user_pref)\n",
    "all_embeddings_from_filtered_data_array = np.array(all_embeddings_from_filtered_data)\n",
    "\n",
    "user_for_example = 1\n",
    "\n",
    "# Taking first user as example to calculate the cosine_similarity\n",
    "user_embedding_example = all_embeddings_from_user_pref_array[user_for_example].reshape(1, -1)  # Asegurar forma correcta para cosine_similarity\n",
    "\n",
    "# To calculate similarity between the user example embeding and the whole content from filtered data\n",
    "content_similarities = cosine_similarity(user_embedding_example, all_embeddings_from_filtered_data_array).flatten()\n",
    "\n",
    "# Sort indexes by similarity\n",
    "most_similar_indexes = content_similarities.argsort()[::-1]\n",
    "\n",
    "# Top-10\n",
    "topten_most_similar_indexes = most_similar_indexes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To display the most similar indexes and their similarity scores\n",
    "print(\"Most similar indexes:\", most_similar_indexes[:10])\n",
    "print(\"Highest similarities:\", content_similarities[most_similar_indexes[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search most_similar_indexes, and preferences, and get recommendations\n",
    "user_id = user_pref.loc[user_for_example]['userId']\n",
    "movies_preferred = user_pref[user_pref['userId']==user_id]['TitulosPeliculas']\n",
    "series_preferred = user_pref[user_pref['userId']==user_id]['TitulosSeries']\n",
    "\n",
    "# Displaying Preferences & Recommendations\n",
    "print(f'''User {user_id} Preferences:\n",
    "      ''')\n",
    "print(f'''Movies preference:''')\n",
    "for movie in movies_preferred.iloc[0].split(';'):\n",
    "    print(f'      {movie.strip()}')\n",
    "\n",
    "print(f'''TV Shows preference:''')\n",
    "for series in series_preferred.iloc[0].split(';'):\n",
    "    print(f'      {series.strip()}')\n",
    "\n",
    "print(f'''Recomendations for user: {user_id}\n",
    "      ''')\n",
    "\n",
    "recomendations_user = filtered_data.loc[topten_most_similar_indexes]['CleanTitle']\n",
    "for recommendation in recomendations_user:\n",
    "    print(f'      {recommendation}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
