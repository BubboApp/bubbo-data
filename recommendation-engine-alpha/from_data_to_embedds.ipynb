{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environmental Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import boto3\n",
    "import firebase_admin\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from botocore.exceptions import ClientError\n",
    "from decimal import Decimal\n",
    "from dotenv import load_dotenv\n",
    "from firebase_admin import credentials, firestore\n",
    "from requests.exceptions import ReadTimeout\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploading the environment variables and get the API key\n",
    "load_dotenv()\n",
    "HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "AWS_ACCESS_KEY = os.getenv(\"AWS_ACCESS_KEY\")\n",
    "AWS_SECRET_KEY = os.getenv(\"AWS_SECRET_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Firebase \n",
    "if not firebase_admin._apps:\n",
    "    cred_path = r'../../../bubbo-dfba0-firebase-adminsdk-fbsvc-79dc4511e7.json'  \n",
    "    cred = credentials.Certificate(cred_path)\n",
    "    firebase_admin.initialize_app(cred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MOVIES AND SERIES CONTENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CON ESTO TRAEMOS LOS CONTENIDOS DE MOVIES/SERIES EN 'filtered_data', LO LIMPIAMOS \n",
    "# Y PREPARAMOS PARA LOS EMBEDDINGS, \n",
    "# Y TRAEMOS GENEROS EN 'df_genres' \n",
    "\n",
    "# Firestore conexion and db collection name\n",
    "db = firestore.client()\n",
    "collection_ref = db.collection('Data_Clean') # \n",
    "docs = collection_ref.stream()\n",
    "data = [{**doc.to_dict(), 'id': doc.id} for doc in docs]\n",
    "df = pd.DataFrame(data)\n",
    "filtered_data = df \n",
    "filtered_data = filtered_data.replace(\"\",pd.NA)\n",
    "filtered_data = filtered_data.dropna()                                                                \n",
    "filtered_data = filtered_data.drop_duplicates()\n",
    "filtered_data = filtered_data.drop(columns='id')\n",
    "filtered_data['ID'] = filtered_data['ID'].astype(str)\n",
    "\n",
    "# Making the sentences to embed \n",
    "filtered_data['sentences_to_embed'] = (\n",
    "    filtered_data.CleanTitle.fillna('') +\n",
    "    filtered_data.Synopsis.fillna('') +\n",
    "    filtered_data.Genre.fillna('').apply(\n",
    "        lambda x: ', '.join(ast.literal_eval(x)) if x.startswith('[') and x.endswith(']') else x ) +\n",
    "    filtered_data.Cast.fillna('') +\n",
    "    filtered_data.Directors.fillna('')\n",
    ")\n",
    "\n",
    "# Genres INFO\n",
    "collection_ref_2 = db.collection('Genres_DB') \n",
    "docs_2 = collection_ref_2.stream()\n",
    "data_2 = [{**doc.to_dict(), 'id': doc.id} for doc in docs_2]\n",
    "df_2 = pd.DataFrame(data_2)\n",
    "df_genres = df_2\n",
    "df_genres.rename(columns={'id':'genero_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Preferences CONTENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracción de Datos de user_preference (por el momento de dynamodb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the info from DynamoDB, user preferences\n",
    "CONFIG = {\n",
    "    'aws': {\n",
    "        'access_key': AWS_ACCESS_KEY,\n",
    "        'secret_key': AWS_SECRET_KEY,\n",
    "        'region': 'eu-west-3',\n",
    "        'table': 'User-7kkcm5dn2rb77hst5nh7gbdisa-staging'\n",
    "    },\n",
    "    'columns': ['userId', 'favoriteMoviesIds', 'favoriteGenresIds', 'favoriteSeriesIds'],\n",
    "}\n",
    "\n",
    "# conexion\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=CONFIG['aws']['access_key'],\n",
    "    aws_secret_access_key=CONFIG['aws']['secret_key'],\n",
    "    region_name=CONFIG['aws']['region']\n",
    ")\n",
    "\n",
    "table = session.resource('dynamodb').Table(CONFIG['aws']['table'])\n",
    "\n",
    "# Values to String\n",
    "def _process_value(value):\n",
    "    if isinstance(value, Decimal):\n",
    "        return str(int(value))\n",
    "    return str(value)\n",
    "\n",
    "# Retrive info from DynamoDB and gets a DataFrame\n",
    "def fetch_preferences():\n",
    "    try:\n",
    "        items = []\n",
    "        start_key = None\n",
    "\n",
    "        while True:\n",
    "            # scan with defined 'columns'  in previous 'CONFIG'\n",
    "            scan_params = {\n",
    "                'ProjectionExpression': ', '.join(CONFIG['columns'])\n",
    "            }\n",
    "            if start_key:\n",
    "                scan_params['ExclusiveStartKey'] = start_key\n",
    "\n",
    "            response = table.scan(**scan_params)\n",
    "            items.extend(response.get('Items', []))\n",
    "\n",
    "            # check for next pages\n",
    "            start_key = response.get('LastEvaluatedKey')\n",
    "            if not start_key:\n",
    "                break\n",
    "\n",
    "        # data extracted processing\n",
    "        processed_data = [{\n",
    "            'userId': _process_value(item.get('userId', '')),\n",
    "            'favoriteMoviesIds': ';'.join(map(_process_value, item.get('favoriteMoviesIds', []))),     ###################### DIRECTOR MAS CAST HAY QUE TRAER DESPUES CUANDO COMPLETO EL DF LUEGO DE FILTERED_DATA\n",
    "            'favoriteGenresIds': ';'.join(map(_process_value, item.get('favoriteGenresIds', []))),\n",
    "            'favoriteSeriesIds': ';'.join(map(_process_value, item.get('favoriteSeriesIds', [])))\n",
    "        } for item in items]\n",
    "\n",
    "        df = pd.DataFrame(processed_data)\n",
    "        return df\n",
    "\n",
    "    except ClientError as e:\n",
    "        print(f\"Error al conectar con DynamoDB: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# calling function to get the df\n",
    "user_pref = fetch_preferences()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming 'user_pref' to our purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limpio el dataframe dejando solo users con genero, movie_favs y tvshow_favs\n",
    "user_pref = user_pref[user_pref['userId'].str.len()==36]\n",
    "user_pref = user_pref.replace(\"\",pd.NA)\n",
    "user_pref = user_pref.dropna()             \n",
    "user_pref.reset_index(inplace=True,drop=True)\n",
    "\n",
    "# convertir los valores en listas para expandirlos con explode\n",
    "user_pref['favoriteGenresIds'] = user_pref['favoriteGenresIds'].apply(lambda x: x.split(';'))\n",
    "user_pref['favoriteMoviesIds'] = user_pref['favoriteMoviesIds'].apply(lambda x: x.split(';'))\n",
    "user_pref['favoriteSeriesIds'] = user_pref['favoriteSeriesIds'].apply(lambda x: x.split(';'))\n",
    "\n",
    "# expandir preferencias de favoriteMoviesIds, favoriteGenresIds, y favoriteSeriesIds por userId\n",
    "user_fav_genres = user_pref[['userId','favoriteGenresIds']].explode('favoriteGenresIds')\n",
    "user_fav_movies = user_pref[['userId','favoriteMoviesIds']].explode('favoriteMoviesIds')\n",
    "user_fav_series = user_pref[['userId','favoriteSeriesIds']].explode('favoriteSeriesIds')\n",
    "\n",
    "\n",
    "# merge para traerme los CleanTitle, Synopsis, 'Genre'\n",
    "user_fav_genres['favoriteGenresIds'] = user_fav_genres['favoriteGenresIds']#.astype(int)          # <- Nuevo agregado# <- Nuevo agregado# <- Nuevo agregado VER ESTO EN LOS EMB\n",
    "user_fav_genres = user_fav_genres.merge(df_genres[['genero_id','genero_name']], left_on='favoriteGenresIds', right_on='genero_id') \n",
    "\n",
    "\n",
    "filtered_data = filtered_data.dropna(subset=['ID'])                                                                                    # <- Nuevo agregado# <- Nuevo agregado# <- Nuevo agregado\n",
    "filtered_data['ID'] = filtered_data['ID'].astype(str).str.strip()                                                                                 # <- Nuevo agregado# <- Nuevo agregado# <- Nuevo agregado\n",
    "user_fav_movies['favoriteMoviesIds'] = user_fav_movies['favoriteMoviesIds'].astype(str).str.strip()\n",
    "user_fav_series['favoriteSeriesIds'] = user_fav_series['favoriteSeriesIds'].astype(str).str.strip()\n",
    "\n",
    "user_fav_movies = user_fav_movies.merge(filtered_data[['ID','CleanTitle','Synopsis', 'Cast', 'Directors']], left_on='favoriteMoviesIds', right_on='ID', how='left')  ###### en esta y lasig fila agregué synopsis\n",
    "user_fav_series = user_fav_series.merge(filtered_data[['ID','CleanTitle','Synopsis', 'Cast', 'Directors']], left_on='favoriteSeriesIds', right_on='ID', how='left')\n",
    "\n",
    "user_fav_genres = user_fav_genres.drop(columns='genero_id')\n",
    "user_fav_genres.rename(columns={'genero_name':'Genres'}, inplace=True)\n",
    "user_fav_movies = user_fav_movies.drop(columns='ID')\n",
    "user_fav_movies.rename(columns={'CleanTitle':'Movies_Titles', 'Synopsis':'Movies_Synopsis', 'Cast':'Movies_Cast', 'Directors':'Movies_Directors'}, inplace=True)\n",
    "user_fav_series = user_fav_series.drop(columns='ID')\n",
    "user_fav_series.rename(columns={'CleanTitle':'Series_Titles', 'Synopsis':'Series_Synopsis', 'Cast':'Series_Cast', 'Directors':'Series_Directors'}, inplace=True)\n",
    "\n",
    "# reAGRUPO por userId para que me queden las listas CleanTitle, Synopsis, 'Genre' por user segun sus favoriteMoviesIds, favoriteGenresIds, y favoriteSeriesIds por userId\n",
    "user_fav_genres = user_fav_genres.groupby('userId')[['favoriteGenresIds','Genres']].agg(list).reset_index()\n",
    "user_fav_movies = user_fav_movies.groupby('userId')[['favoriteMoviesIds','Movies_Titles', 'Movies_Synopsis', 'Movies_Cast', 'Movies_Directors']].agg(list).reset_index()\n",
    "user_fav_series = user_fav_series.groupby('userId')[['favoriteSeriesIds','Series_Titles', 'Series_Synopsis', 'Series_Cast', 'Series_Directors']].agg(list).reset_index()\n",
    "\n",
    "#termino de acomodar 'user_pref' para dar paso a los embeddings\n",
    "user_pref = user_pref.merge(user_fav_genres, left_on='userId', right_on='userId').drop(columns=['favoriteGenresIds_y'])\n",
    "user_pref.rename(columns={'favoriteGenresIds_x':'favoriteGenresIds'},inplace=True)\n",
    "user_pref = user_pref.merge(user_fav_movies, left_on='userId', right_on='userId').drop(columns=['favoriteMoviesIds_y'])\n",
    "user_pref.rename(columns={'favoriteMoviesIds_x':'favoriteMoviesIds'},inplace=True)\n",
    "user_pref = user_pref.merge(user_fav_series, left_on='userId', right_on='userId').drop(columns=['favoriteSeriesIds_y'])\n",
    "user_pref.rename(columns={'favoriteSeriesIds_x':'favoriteSeriesIds'},inplace=True)\n",
    "user_pref = user_pref.reindex(['userId', 'favoriteGenresIds', 'Genres', 'favoriteMoviesIds', 'Movies_Titles','Movies_Synopsis', 'Movies_Cast', 'Movies_Directors', 'favoriteSeriesIds', 'Series_Titles', 'Series_Synopsis', 'Series_Cast', 'Series_Directors'], axis=1)\n",
    "\n",
    "# Sentences we want to be embedded from user_preferences MOVIES\n",
    "user_pref['movies_sentences_to_embed'] = (user_pref.Movies_Titles.fillna('') +\n",
    "                                   user_pref.Movies_Synopsis.fillna('')+\n",
    "                                   user_pref.Genres.fillna('') +\n",
    "                                   user_pref.Movies_Cast.fillna('') +\n",
    "                                   user_pref.Movies_Directors.fillna(''))\n",
    "\n",
    "# Sentences we want to be embedded from user_preferences SERIES\n",
    "user_pref['series_sentences_to_embed'] = (user_pref.Series_Titles.fillna('') + \n",
    "                                   user_pref.Series_Synopsis.fillna('') +\n",
    "                                   user_pref.Genres.fillna('') +\n",
    "                                   user_pref.Series_Cast.fillna('') +\n",
    "                                   user_pref.Series_Directors.fillna(''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Sentences for vectorize from 'filtered_data' and 'user_pref==Movies' & 'user_pref==Series'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOS ID PARA EL INDEX Y LAS SENTENCES A FORMATO PARA PEDIR LOS EMBEDDINGS\n",
    "\n",
    "# De filtered_data (todos los contenidos)\n",
    "ids_from_filtered_data = filtered_data['ID'].tolist()  # Guardamos los IDs\n",
    "sentences_from_filtered_data = filtered_data['sentences_to_embed'].dropna().astype(str).tolist()\n",
    "\n",
    "# De user_pref dividido en movies y series\n",
    "# Guardo userId junto con la sentence a vectorizar\n",
    "movies_sentences_from_user_pref = user_pref[['userId', 'movies_sentences_to_embed']].dropna().astype(str)\n",
    "series_sentences_from_user_pref = user_pref[['userId', 'series_sentences_to_embed']].dropna().astype(str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batches to send to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the model on Hugging Face processes only requests that can be completed within 60 seconds, \n",
    "# we need to divide the sentences into batches.\n",
    "def split_into_batches(sentences, batch_size):\n",
    "    return [sentences[i:i + batch_size] for i in range(0, len(sentences), batch_size)]\n",
    "\n",
    "# After trying with different values, we've reach the maximum batch size to get response succesfully\n",
    "batches = split_into_batches(sentences_from_filtered_data, 50)\n",
    "\n",
    "# We split the sentences in batches as we did previously with filtered_data\n",
    "movies_batches_user_pref = split_into_batches(movies_sentences_from_user_pref, 50)\n",
    "series_batches_user_pref = split_into_batches(series_sentences_from_user_pref, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Access to HF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clave de API cargada correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Check key availability\n",
    "if HUGGINGFACE_API_KEY is None:\n",
    "    print(\"Error: No se encontró la clave de API de Hugging Face.\")\n",
    "else:\n",
    "    print(\"Clave de API cargada correctamente.\")\n",
    "\n",
    "# Model URL\n",
    "API_URL = \"https://api-inference.huggingface.co/pipeline/feature-extraction/sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# API header and key\n",
    "headers = {\"Authorization\": f\"Bearer {HUGGINGFACE_API_KEY}\"}  \n",
    "\n",
    "# Function to get embeddings from Hugging Face API\n",
    "def get_embeddings_from_api(sentences):\n",
    "    url = API_URL\n",
    "    payload = {\"inputs\": sentences}\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=payload, timeout=10)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error {response.status_code}: {response.text}\")\n",
    "        return None\n",
    "    \n",
    "# No prestar atencion nadie mas que Agus // esta aqui para cuando se completen los 200k que funcionen ok y esten almacenados para lograr obtener \n",
    "# los nuevos embeedings validos\n",
    "\n",
    "# all_embeddings_from_filtered_data = []\n",
    "# for batch in batches:\n",
    "#     print(f\"Processing batch with {len(batch)} sentences...\")\n",
    "#     time.sleep(7)\n",
    "#     embeddings = get_embeddings_from_api(batch)\n",
    "#     if embeddings:\n",
    "#         all_embeddings_from_filtered_data.extend(embeddings)\n",
    "\n",
    "# # Asociamos cada embedding con su respectivo ID\n",
    "# all_embeddings_dict = {id_: emb for id_, emb in zip(ids_from_filtered_data, all_embeddings_from_filtered_data)}\n",
    "\n",
    "# print(\"Embeddings processed successfully:\")\n",
    "# print(list(all_embeddings_dict.items())[:2])  # Muestra los primeros pares ID - embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sending Sentences to vectorize for HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MOVIES\n",
    "movies_embeddings_dict = {}  # Diccionario para almacenar {userId: embedding}\n",
    "num_batches = len(movies_batches_user_pref)  # cuantas batches tengo\n",
    "\n",
    "for i, batch in enumerate(movies_batches_user_pref, start=1):\n",
    "    batch_user_ids = batch['userId'].tolist()\n",
    "    batch_sentences = batch['movies_sentences_to_embed'].tolist()\n",
    "    \n",
    "    while True:\n",
    "        try: \n",
    "            print(f\"Processing movies batch {i}/{num_batches} with {len(batch)} sentences...\")\n",
    "            time.sleep(1)\n",
    "            embeddings = get_embeddings_from_api(batch_sentences)\n",
    "            \n",
    "            if embeddings:\n",
    "                movies_embeddings_dict.update({uid: emb for uid, emb in zip(batch_user_ids, embeddings)})\n",
    "                break\n",
    "        except ReadTimeout:\n",
    "            print(f'Timeout on batch {i}. Retrying...')\n",
    "            time.sleep(5)\n",
    "\n",
    "print(\"Movies Embeddings processed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Series\n",
    "series_embeddings_dict = {}  # Diccionario para almacenar {userId: embedding}\n",
    "num_batches_series = len(series_batches_user_pref)  # Total de batches\n",
    "\n",
    "for i, batch in enumerate(series_batches_user_pref, start=1):\n",
    "    batch_user_ids = batch['userId'].tolist()\n",
    "    batch_sentences = batch['series_sentences_to_embed'].tolist()\n",
    "    \n",
    "    while True:  # Intentar hasta que se procese correctamente\n",
    "        try:\n",
    "            print(f\"Processing series batch {i}/{num_batches_series} with {len(batch)} sentences...\")\n",
    "            time.sleep(1)\n",
    "            embeddings = get_embeddings_from_api(batch_sentences)\n",
    "            \n",
    "            if embeddings:\n",
    "                series_embeddings_dict.update({uid: emb for uid, emb in zip(batch_user_ids, embeddings)})\n",
    "                break  # Salir del bucle si se procesó correctamente\n",
    "        except ReadTimeout:\n",
    "            print(f\"Timeout on batch {i}. Retrying...\")\n",
    "            time.sleep(5)  # Esperar antes de reintentar\n",
    "\n",
    "print(\"Series Embeddings processed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### getting local similarities for testing purposes, but awaiting for availability of 200k well-fed titles to obtain new effective embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO USAR ESTO - SOLO LO MANTENGO PARA TRAER LOS EMBEDDINGS EN CSV PARA HACER PRUEBAS HASTA QUE\n",
    "# TENGAMOS LOS NUEVOS EMBEDDINGS DE LOS CONTENIDOS CARGADOS EN FIREBASE/VERTEX\n",
    "\n",
    "\n",
    "# Genres database, local by now. Then we have to get them linked to firebase, or wathever\n",
    "all_embeddings_from_filtered_data = pd.read_csv(r'../../../all_content_embeddings.csv')\n",
    "\n",
    "# Conversión optimizada para el dict de los embeddings que habiamos guardado en csv\n",
    "def fast_convert(emb):\n",
    "    if isinstance(emb, str): \n",
    "        return np.array(json.loads(emb), dtype=np.float32)  # Usa float32 para ahorrar memoria\n",
    "    return emb\n",
    "\n",
    "all_embeddings_dict = {\n",
    "    id_: fast_convert(emb)\n",
    "    for id_, emb in zip(\n",
    "        all_embeddings_from_filtered_data['ID'], \n",
    "        all_embeddings_from_filtered_data['Embedding']\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import pickle\n",
    "\n",
    "\n",
    "movies_embeddings = r'example_files_to_delete/movies_embeddings.faiss'\n",
    "movies_user_ids = r'example_files_to_delete/movies_user_ids.pkl'\n",
    "series_embeddings = r'example_files_to_delete/series_embeddings.faiss'\n",
    "series_user_ids = r'example_files_to_delete/series_user_ids.pkl'\n",
    "\n",
    "\n",
    "# Cargar los embeddings y los user ids para las películas\n",
    "def load_movie_data(embedding_file, user_ids_file):\n",
    "    # Cargar embeddings de películas\n",
    "    movie_embeddings = faiss.read_index(embedding_file)\n",
    "    \n",
    "    # Cargar user ids de películas\n",
    "    with open(user_ids_file, 'rb') as f:\n",
    "        movie_user_ids = pickle.load(f)\n",
    "    \n",
    "    return movie_embeddings, movie_user_ids\n",
    "\n",
    "# Cargar los embeddings y los user ids para las series\n",
    "def load_series_data(embedding_file, user_ids_file):\n",
    "    # Cargar embeddings de series\n",
    "    series_embeddings = faiss.read_index(embedding_file)\n",
    "    \n",
    "    # Cargar user ids de series\n",
    "    with open(user_ids_file, 'rb') as f:\n",
    "        series_user_ids = pickle.load(f)\n",
    "    \n",
    "    return series_embeddings, series_user_ids\n",
    "\n",
    "# Crear un diccionario con los embeddings para películas\n",
    "def create_embeddings_dict(user_ids, embeddings):\n",
    "    embeddings_dict = {}\n",
    "    \n",
    "    # Convertir los embeddings a una matriz (si es necesario)\n",
    "    embeddings_matrix = embeddings.reconstruct_n(0, embeddings.ntotal)  # Convierte los embeddings en una matriz de numpy\n",
    "    \n",
    "    # Asignar el embedding a cada userId en el diccionario\n",
    "    for user_id, embedding in zip(user_ids, embeddings_matrix):\n",
    "        embeddings_dict[user_id] = embedding.tolist()  # Convertimos el embedding a lista para almacenamiento\n",
    "    \n",
    "    return embeddings_dict\n",
    "\n",
    "# Cargar los datos de películas y series\n",
    "movie_embeddings, movie_user_ids = load_movie_data(movies_embeddings, movies_user_ids)\n",
    "series_embeddings, series_user_ids = load_series_data(series_embeddings, series_user_ids)\n",
    "\n",
    "# Crear los diccionarios con los embeddings\n",
    "movies_embeddings_dict = create_embeddings_dict(movie_user_ids, movie_embeddings)\n",
    "series_embeddings_dict = create_embeddings_dict(series_user_ids, series_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movies Similarities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### >>>>>> NO CORRER ESTA CELDA EN LOCAL PORQUE PUEDE DEMORAR HORAS <<<<<<<< ###\n",
    "\n",
    "# Convertir los embeddings a arrays de numpy para cálculos más rápidos\n",
    "all_embeddings_from_filtered_data_array = np.array(list(all_embeddings_dict.values()))\n",
    "\n",
    "# Diccionarios para almacenar las recomendaciones\n",
    "movies_recommendations_dict = {}\n",
    "\n",
    "# Obtener recomendaciones para cada usuario en movies_embeddings_dict\n",
    "for user_id, user_embedding in movies_embeddings_dict.items():\n",
    "    user_embedding_array = np.array(user_embedding).reshape(1, -1)  # Asegurar la forma correcta\n",
    "    movies_content_similarities = cosine_similarity(user_embedding_array, all_embeddings_from_filtered_data_array).flatten()\n",
    "    \n",
    "    # Ordenar por similitud y seleccionar el top-10\n",
    "    movies_most_similar_indexes = movies_content_similarities.argsort()[::-1][:10]\n",
    "    \n",
    "    # Convertir los índices en IDs reales\n",
    "    movies_recommended_ids = [filtered_data.iloc[i]['ID'] for i in movies_most_similar_indexes]\n",
    "    \n",
    "    # Guardar en el diccionario\n",
    "    movies_recommendations_dict[user_id] = movies_recommended_ids\n",
    "\n",
    "\n",
    "# Mostrar ejemplos\n",
    "print(\"Ejemplo de recomendaciones para un usuario en Movies:\")\n",
    "example_user = list(movies_recommendations_dict.keys())[0]\n",
    "print(f\"Usuario: {example_user} - Recomendaciones: {movies_recommendations_dict[example_user]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tv Shows (Series) Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### >>>>>> NO CORRER ESTA CELDA EN LOCAL PORQUE PUEDE DEMORAR HORAS <<<<<<<< ###\n",
    "# dict para las recomendaciones\n",
    "series_recommendations_dict = {}\n",
    "\n",
    "number_target_series_recommend = 25\n",
    "\n",
    "# recommend's para cada usuario en series_embeddings_dict\n",
    "for user_id, user_embedding in series_embeddings_dict.items():\n",
    "    user_embedding_array = np.array(user_embedding).reshape(1, -1)  # Asegurar la forma correcta\n",
    "    series_content_similarities = cosine_similarity(user_embedding_array, all_embeddings_from_filtered_data_array).flatten()\n",
    "    \n",
    "    # orden por similitud y top-goal\n",
    "    series_most_similar_indexes = series_content_similarities.argsort()[::-1][:number_target_series_recommend]\n",
    "    \n",
    "    # paso los indices a IDs reales\n",
    "    series_recommended_ids = [filtered_data.iloc[i]['ID'] for i in series_most_similar_indexes]\n",
    "    \n",
    "    # Guardar en el diccionario\n",
    "    series_recommendations_dict[user_id] = series_recommended_ids\n",
    "\n",
    "# Mostrar ejemplos\n",
    "print(\"\\nEjemplo de recomendaciones para un usuario en Series:\")\n",
    "example_user = list(series_recommendations_dict.keys())[0]\n",
    "print(f\"Usuario: {example_user} - Recomendaciones: {series_recommendations_dict[example_user]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recomendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardo\n",
    "with open(\"movies_recommendations.json\", \"w\") as f:\n",
    "    json.dump(movies_recommendations_dict, f, indent=4)\n",
    "\n",
    "with open(\"series_recommendations.json\", \"w\") as f:\n",
    "    json.dump(series_recommendations_dict, f, indent=4)\n",
    "\n",
    "print(\"Recomendaciones guardadas en JSON\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
