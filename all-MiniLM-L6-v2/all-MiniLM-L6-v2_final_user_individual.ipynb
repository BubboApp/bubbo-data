{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import boto3\n",
    "import faiss\n",
    "import firebase_admin\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "from decimal import Decimal\n",
    "from dotenv import load_dotenv\n",
    "from firebase_admin import credentials, firestore\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "#from torch.nn.functional import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploading the environment variables and get the API key\n",
    "load_dotenv()\n",
    "HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "AWS_ACCESS_KEY = os.getenv(\"AWS_ACCESS_KEY\")\n",
    "AWS_SECRET_KEY = os.getenv(\"AWS_SECRET_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genres database, local by now. Then we have to get them linked to firebase, or wathever\n",
    "df_genres = pd.read_csv(r'generos.csv', sep=',')\n",
    "\n",
    "all_embeddings_from_filtered_data = pd.read_csv(r'../../../all_content_embeddings.csv')\n",
    "filtered_data = pd.read_csv(r'../../../Test_clean.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Conversión optimizada para el dict de los embeddings que habiamos guardado en csv\n",
    "def fast_convert(emb):\n",
    "    if isinstance(emb, str): \n",
    "        return np.array(json.loads(emb), dtype=np.float32)  # Usa float32 para ahorrar memoria\n",
    "    return emb\n",
    "\n",
    "all_embeddings_dict = {\n",
    "    id_: fast_convert(emb)\n",
    "    for id_, emb in zip(\n",
    "        all_embeddings_from_filtered_data['ID'], \n",
    "        all_embeddings_from_filtered_data['Embedding']\n",
    "    )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Data from FireBase (remains missing the conextion to firebase collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Firebase if it's not done (I KEEP THIS FOR THE MOMENT THE DICT OF EMBEDDIGNS WOULD BE A COLLECTION OF)\n",
    "if not firebase_admin._apps:\n",
    "    cred_path = r'../../../bubbo-dfba0-firebase-adminsdk-fbsvc-79dc4511e7.json'  \n",
    "    cred = credentials.Certificate(cred_path)\n",
    "    firebase_admin.initialize_app(cred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_UnaryStreamMultiCallable' object has no attribute '_retry'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_MultiThreadedRendezvous\u001b[0m                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Agustín\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:116\u001b[0m, in \u001b[0;36m_StreamingResponseIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrapped\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# If the stream has already returned data, we cannot recover here.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Agustín\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\grpc\\_channel.py:543\u001b[0m, in \u001b[0;36m_Rendezvous.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Agustín\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\grpc\\_channel.py:969\u001b[0m, in \u001b[0;36m_MultiThreadedRendezvous._next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 969\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[1;31m_MultiThreadedRendezvous\u001b[0m: <_MultiThreadedRendezvous of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"Query timed out. Please try either limiting the entities scanned, or run with an updated index configuration.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:142.250.200.74:443 {created_time:\"2025-02-14T19:51:56.7229135+00:00\", grpc_status:14, grpc_message:\"Query timed out. Please try either limiting the entities scanned, or run with an updated index configuration.\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mServiceUnavailable\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Agustín\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\cloud\\firestore_v1\\query.py:411\u001b[0m, in \u001b[0;36mQuery._make_stream\u001b[1;34m(self, transaction, retry, timeout, explain_options)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 411\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresponse_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mGoogleAPICallError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32mc:\\Users\\Agustín\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\api_core\\grpc_helpers.py:119\u001b[0m, in \u001b[0;36m_StreamingResponseIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# If the stream has already returned data, we cannot recover here.\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[1;31mServiceUnavailable\u001b[0m: 503 Query timed out. Please try either limiting the entities scanned, or run with an updated index configuration.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[252], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m docs \u001b[38;5;241m=\u001b[39m collection_ref\u001b[38;5;241m.\u001b[39mstream()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# documents to dictionaries\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     10\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data)\n",
      "File \u001b[1;32mc:\\Users\\Agustín\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\cloud\\firestore_v1\\stream_generator.py:58\u001b[0m, in \u001b[0;36mStreamGenerator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     60\u001b[0m         \u001b[38;5;66;03m# If explain_metrics is available, it would be returned.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mvalue:\n",
      "File \u001b[1;32mc:\\Users\\Agustín\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\cloud\\firestore_v1\\query.py:413\u001b[0m, in \u001b[0;36mQuery._make_stream\u001b[1;34m(self, transaction, retry, timeout, explain_options)\u001b[0m\n\u001b[0;32m    411\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(response_iterator, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mGoogleAPICallError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m--> 413\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_query_after_exception\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransaction\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    414\u001b[0m         new_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_after(last_snapshot)\n\u001b[0;32m    415\u001b[0m         response_iterator, _ \u001b[38;5;241m=\u001b[39m new_query\u001b[38;5;241m.\u001b[39m_get_stream_iterator(\n\u001b[0;32m    416\u001b[0m             transaction,\n\u001b[0;32m    417\u001b[0m             retry,\n\u001b[0;32m    418\u001b[0m             timeout,\n\u001b[0;32m    419\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Agustín\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\cloud\\firestore_v1\\query.py:264\u001b[0m, in \u001b[0;36mQuery._retry_query_after_exception\u001b[1;34m(self, exc, retry, transaction)\u001b[0m\n\u001b[0;32m    262\u001b[0m         transport \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39m_firestore_api\u001b[38;5;241m.\u001b[39m_transport\n\u001b[0;32m    263\u001b[0m         gapic_callable \u001b[38;5;241m=\u001b[39m transport\u001b[38;5;241m.\u001b[39mrun_query\n\u001b[1;32m--> 264\u001b[0m         retry \u001b[38;5;241m=\u001b[39m \u001b[43mgapic_callable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry\u001b[49m\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m retry\u001b[38;5;241m.\u001b[39m_predicate(exc)\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_UnaryStreamMultiCallable' object has no attribute '_retry'"
     ]
    }
   ],
   "source": [
    "'''# Esta celda por el momento tampoco sirve si se obtienen los datos local\n",
    "\n",
    "# Firestore conexion and db collection name\n",
    "db = firestore.client()\n",
    "collection_ref = db.collection('Data_Clean') # Look for the new collection\n",
    "# to get it all\n",
    "docs = collection_ref.stream()\n",
    "# documents to dictionaries\n",
    "data = [{**doc.to_dict(), 'id': doc.id} for doc in docs]\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 401689 entries, 0 to 403139\n",
      "Data columns (total 10 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   Unnamed: 0    401689 non-null  int64 \n",
      " 1   ID            401689 non-null  object\n",
      " 2   Genre         401689 non-null  object\n",
      " 3   CleanTitle    401689 non-null  object\n",
      " 4   Synopsis      401689 non-null  object\n",
      " 5   Directors     401689 non-null  object\n",
      " 6   Cast          401689 non-null  object\n",
      " 7   Type          401689 non-null  object\n",
      " 8   PlatformName  401689 non-null  object\n",
      " 9   Score         401689 non-null  int64 \n",
      "dtypes: int64(2), object(8)\n",
      "memory usage: 33.7+ MB\n"
     ]
    }
   ],
   "source": [
    "# Cuando corro esta celda, \"reseteo\" a filtered_data\n",
    "#filtered_data = df # Comentada porque ahora la saco de local ya que los embeddings ya estan \n",
    "filtered_data = filtered_data.replace(\"\",pd.NA)\n",
    "filtered_data = filtered_data.dropna()                                                                \n",
    "filtered_data = filtered_data.drop_duplicates()\n",
    "filtered_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>ID</th>\n",
       "      <th>Genre</th>\n",
       "      <th>CleanTitle</th>\n",
       "      <th>Synopsis</th>\n",
       "      <th>Directors</th>\n",
       "      <th>Cast</th>\n",
       "      <th>Type</th>\n",
       "      <th>PlatformName</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2602</th>\n",
       "      <td>3111</td>\n",
       "      <td>671</td>\n",
       "      <td>Adventure</td>\n",
       "      <td>Harry Potter 1: en de Steen der Wijzen</td>\n",
       "      <td>Dit is de betoverende verfilming gebaseerd op ...</td>\n",
       "      <td>Chris Columbus</td>\n",
       "      <td>Daniel Radcliffe; Rupert Grint; Emma Watson; R...</td>\n",
       "      <td>Movie</td>\n",
       "      <td>Rakuten TV</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30262</th>\n",
       "      <td>35868</td>\n",
       "      <td>671</td>\n",
       "      <td>Children</td>\n",
       "      <td>Harry Potter and the Philosopher's Stone</td>\n",
       "      <td>Based on the wildly popular J.K. Rowling's boo...</td>\n",
       "      <td>Chris Columbus</td>\n",
       "      <td>Emma Watson; Daniel Radcliffe; Rupert Grint</td>\n",
       "      <td>Movie</td>\n",
       "      <td>Amazon Prime Video</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0   ID      Genre                                CleanTitle  \\\n",
       "2602         3111  671  Adventure    Harry Potter 1: en de Steen der Wijzen   \n",
       "30262       35868  671   Children  Harry Potter and the Philosopher's Stone   \n",
       "\n",
       "                                                Synopsis       Directors  \\\n",
       "2602   Dit is de betoverende verfilming gebaseerd op ...  Chris Columbus   \n",
       "30262  Based on the wildly popular J.K. Rowling's boo...  Chris Columbus   \n",
       "\n",
       "                                                    Cast   Type  \\\n",
       "2602   Daniel Radcliffe; Rupert Grint; Emma Watson; R...  Movie   \n",
       "30262        Emma Watson; Daniel Radcliffe; Rupert Grint  Movie   \n",
       "\n",
       "             PlatformName  Score  \n",
       "2602           Rakuten TV      6  \n",
       "30262  Amazon Prime Video      6  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data[filtered_data['ID']=='671']                                                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Preferences from DynamoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the info from DynamoDB, user preferences\n",
    "CONFIG = {\n",
    "    'aws': {\n",
    "        'access_key': AWS_ACCESS_KEY,\n",
    "        'secret_key': AWS_SECRET_KEY,\n",
    "        'region': 'eu-west-3',\n",
    "        'table': 'User-7kkcm5dn2rb77hst5nh7gbdisa-staging'\n",
    "    },\n",
    "    'columns': ['userId', 'favoriteMoviesIds', 'favoriteGenresIds', 'favoriteSeriesIds'],\n",
    "}\n",
    "\n",
    "# conexion\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=CONFIG['aws']['access_key'],\n",
    "    aws_secret_access_key=CONFIG['aws']['secret_key'],\n",
    "    region_name=CONFIG['aws']['region']\n",
    ")\n",
    "\n",
    "table = session.resource('dynamodb').Table(CONFIG['aws']['table'])\n",
    "\n",
    "# Values to String\n",
    "def _process_value(value):\n",
    "    if isinstance(value, Decimal):\n",
    "        return str(int(value))\n",
    "    return str(value)\n",
    "\n",
    "# Retrive info from DynamoDB and gets a DataFrame\n",
    "def fetch_preferences():\n",
    "    try:\n",
    "        items = []\n",
    "        start_key = None\n",
    "\n",
    "        while True:\n",
    "            # scan with defined 'columns'  in previous 'CONFIG'\n",
    "            scan_params = {\n",
    "                'ProjectionExpression': ', '.join(CONFIG['columns'])\n",
    "            }\n",
    "            if start_key:\n",
    "                scan_params['ExclusiveStartKey'] = start_key\n",
    "\n",
    "            response = table.scan(**scan_params)\n",
    "            items.extend(response.get('Items', []))\n",
    "\n",
    "            # check for next pages\n",
    "            start_key = response.get('LastEvaluatedKey')\n",
    "            if not start_key:\n",
    "                break\n",
    "\n",
    "        # data extracted processing\n",
    "        processed_data = [{\n",
    "            'userId': _process_value(item.get('userId', '')),\n",
    "            'favoriteMoviesIds': ';'.join(map(_process_value, item.get('favoriteMoviesIds', []))),     ###################### DIRECTOR MAS CAST HAY QUE TRAER DESPUES CUANDO COMPLETO EL DF LUEGO DE FILTERED_DATA\n",
    "            'favoriteGenresIds': ';'.join(map(_process_value, item.get('favoriteGenresIds', []))),\n",
    "            'favoriteSeriesIds': ';'.join(map(_process_value, item.get('favoriteSeriesIds', [])))\n",
    "        } for item in items]\n",
    "\n",
    "        df = pd.DataFrame(processed_data)\n",
    "        return df\n",
    "\n",
    "    except ClientError as e:\n",
    "        print(f\"Error al conectar con DynamoDB: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# calling function to get the df\n",
    "user_pref = fetch_preferences()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates: 0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9284 entries, 0 to 9283\n",
      "Data columns (total 4 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   userId             9284 non-null   object\n",
      " 1   favoriteMoviesIds  9284 non-null   object\n",
      " 2   favoriteGenresIds  9284 non-null   object\n",
      " 3   favoriteSeriesIds  9284 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 290.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# limpio el dataframe dejando solo users con genero, movie_favs y tvshow_favs\n",
    "user_pref = user_pref[user_pref['userId'].str.len()==36]\n",
    "user_pref = user_pref.replace(\"\",pd.NA)\n",
    "user_pref = user_pref.dropna()             \n",
    "user_pref.reset_index(inplace=True,drop=True)\n",
    "print(f'Duplicates: {user_pref.duplicated().sum()}')\n",
    "user_pref.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>favoriteMoviesIds</th>\n",
       "      <th>favoriteGenresIds</th>\n",
       "      <th>favoriteSeriesIds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>a189607e-3041-70a5-3daf-01cd2118baff</td>\n",
       "      <td>671;672;673;135397;354912</td>\n",
       "      <td>10749;27;9648;18;10770;10766;10767;53;878;1076...</td>\n",
       "      <td>66732;93405;18165;119051;65334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   userId          favoriteMoviesIds  \\\n",
       "101  a189607e-3041-70a5-3daf-01cd2118baff  671;672;673;135397;354912   \n",
       "\n",
       "                                     favoriteGenresIds  \\\n",
       "101  10749;27;9648;18;10770;10766;10767;53;878;1076...   \n",
       "\n",
       "                  favoriteSeriesIds  \n",
       "101  66732;93405;18165;119051;65334  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_pref[user_pref['userId']== 'a189607e-3041-70a5-3daf-01cd2118baff'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora tengo que traer tambien las Cast y Director\n",
    "\n",
    "# convertir los valores en listas para expandirlos con explode\n",
    "user_pref['favoriteGenresIds'] = user_pref['favoriteGenresIds'].apply(lambda x: x.split(';'))\n",
    "user_pref['favoriteMoviesIds'] = user_pref['favoriteMoviesIds'].apply(lambda x: x.split(';'))\n",
    "user_pref['favoriteSeriesIds'] = user_pref['favoriteSeriesIds'].apply(lambda x: x.split(';'))\n",
    "\n",
    "# expandir preferencias de favoriteMoviesIds, favoriteGenresIds, y favoriteSeriesIds por userId\n",
    "user_fav_genres = user_pref[['userId','favoriteGenresIds']].explode('favoriteGenresIds')\n",
    "user_fav_movies = user_pref[['userId','favoriteMoviesIds']].explode('favoriteMoviesIds')\n",
    "user_fav_series = user_pref[['userId','favoriteSeriesIds']].explode('favoriteSeriesIds')\n",
    "\n",
    "\n",
    "# merge para traerme los CleanTitle, Synopsis, 'Genre'\n",
    "user_fav_genres['favoriteGenresIds'] = user_fav_genres['favoriteGenresIds'].astype(int)\n",
    "user_fav_genres = user_fav_genres.merge(df_genres[['genero_id','genero_name']], left_on='favoriteGenresIds', right_on='genero_id') \n",
    "\n",
    "\n",
    "filtered_data = filtered_data.dropna(subset=['ID'])                                                                                    # <- Nuevo agregado# <- Nuevo agregado# <- Nuevo agregado\n",
    "filtered_data['ID'] = filtered_data['ID'].astype(str).str.strip()                                                                                 # <- Nuevo agregado# <- Nuevo agregado# <- Nuevo agregado\n",
    "user_fav_movies['favoriteMoviesIds'] = user_fav_movies['favoriteMoviesIds'].astype(str).str.strip()\n",
    "user_fav_series['favoriteSeriesIds'] = user_fav_series['favoriteSeriesIds'].astype(str).str.strip()\n",
    "\n",
    "user_fav_movies = user_fav_movies.merge(filtered_data[['ID','CleanTitle','Synopsis', 'Cast', 'Directors']], left_on='favoriteMoviesIds', right_on='ID', how='left')  ###### en esta y lasig fila agregué synopsis\n",
    "user_fav_series = user_fav_series.merge(filtered_data[['ID','CleanTitle','Synopsis', 'Cast', 'Directors']], left_on='favoriteSeriesIds', right_on='ID', how='left')\n",
    "\n",
    "user_fav_genres = user_fav_genres.drop(columns='genero_id')\n",
    "user_fav_genres.rename(columns={'genero_name':'Genres'}, inplace=True)\n",
    "user_fav_movies = user_fav_movies.drop(columns='ID')\n",
    "user_fav_movies.rename(columns={'CleanTitle':'Movies_Titles', 'Synopsis':'Movies_Synopsis', 'Cast':'Movies_Cast', 'Directors':'Movies_Directors'}, inplace=True)\n",
    "user_fav_series = user_fav_series.drop(columns='ID')\n",
    "user_fav_series.rename(columns={'CleanTitle':'Series_Titles', 'Synopsis':'Series_Synopsis', 'Cast':'Series_Cast', 'Directors':'Series_Directors'}, inplace=True)\n",
    "\n",
    "# reAGRUPO por userId para que me queden las listas CleanTitle, Synopsis, 'Genre' por user segun sus favoriteMoviesIds, favoriteGenresIds, y favoriteSeriesIds por userId\n",
    "user_fav_genres = user_fav_genres.groupby('userId')[['favoriteGenresIds','Genres']].agg(list).reset_index()\n",
    "user_fav_movies = user_fav_movies.groupby('userId')[['favoriteMoviesIds','Movies_Titles', 'Movies_Synopsis', 'Movies_Cast', 'Movies_Directors']].agg(list).reset_index()\n",
    "user_fav_series = user_fav_series.groupby('userId')[['favoriteSeriesIds','Series_Titles', 'Series_Synopsis', 'Series_Cast', 'Series_Directors']].agg(list).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>favoriteMoviesIds</th>\n",
       "      <th>favoriteGenresIds</th>\n",
       "      <th>favoriteSeriesIds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>a189607e-3041-70a5-3daf-01cd2118baff</td>\n",
       "      <td>[671, 672, 673, 135397, 354912]</td>\n",
       "      <td>[10749, 27, 9648, 18, 10770, 10766, 10767, 53,...</td>\n",
       "      <td>[66732, 93405, 18165, 119051, 65334]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   userId                favoriteMoviesIds  \\\n",
       "101  a189607e-3041-70a5-3daf-01cd2118baff  [671, 672, 673, 135397, 354912]   \n",
       "\n",
       "                                     favoriteGenresIds  \\\n",
       "101  [10749, 27, 9648, 18, 10770, 10766, 10767, 53,...   \n",
       "\n",
       "                        favoriteSeriesIds  \n",
       "101  [66732, 93405, 18165, 119051, 65334]  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_pref[user_pref['userId']== 'a189607e-3041-70a5-3daf-01cd2118baff'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#termino de acomodar 'user_pref' para dar paso a los embeddings\n",
    "user_pref = user_pref.merge(user_fav_genres, left_on='userId', right_on='userId').drop(columns=['favoriteGenresIds_y'])\n",
    "user_pref.rename(columns={'favoriteGenresIds_x':'favoriteGenresIds'},inplace=True)\n",
    "user_pref = user_pref.merge(user_fav_movies, left_on='userId', right_on='userId').drop(columns=['favoriteMoviesIds_y'])\n",
    "user_pref.rename(columns={'favoriteMoviesIds_x':'favoriteMoviesIds'},inplace=True)\n",
    "user_pref = user_pref.merge(user_fav_series, left_on='userId', right_on='userId').drop(columns=['favoriteSeriesIds_y'])\n",
    "user_pref.rename(columns={'favoriteSeriesIds_x':'favoriteSeriesIds'},inplace=True)\n",
    "user_pref = user_pref.reindex(['userId', 'favoriteGenresIds', 'Genres', 'favoriteMoviesIds', 'Movies_Titles','Movies_Synopsis', 'Movies_Cast', 'Movies_Directors', 'favoriteSeriesIds', 'Series_Titles', 'Series_Synopsis', 'Series_Cast', 'Series_Directors'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>favoriteGenresIds</th>\n",
       "      <th>Genres</th>\n",
       "      <th>favoriteMoviesIds</th>\n",
       "      <th>Movies_Titles</th>\n",
       "      <th>Movies_Synopsis</th>\n",
       "      <th>Movies_Cast</th>\n",
       "      <th>Movies_Directors</th>\n",
       "      <th>favoriteSeriesIds</th>\n",
       "      <th>Series_Titles</th>\n",
       "      <th>Series_Synopsis</th>\n",
       "      <th>Series_Cast</th>\n",
       "      <th>Series_Directors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c1d9a05e-50c1-7041-a60e-9a56c092e612</td>\n",
       "      <td>[28, 10759, 12, 16, 10762, 80, 35, 99, 10764, ...</td>\n",
       "      <td>[Acción, Action &amp; Adventure, Aventura, Animaci...</td>\n",
       "      <td>[27205, 157336, 155, 19995, 293660]</td>\n",
       "      <td>[Perfect Match, Interstellar, Interstellar, Th...</td>\n",
       "      <td>[Watch the latest Perfect Match (2018) Full on...</td>\n",
       "      <td>[Feng Zhi Mo; Xu Ding, Matthew McConaughey; Je...</td>\n",
       "      <td>[Ba Chen Xu, Christopher Nolan, Christopher No...</td>\n",
       "      <td>[1399, 71446, 66732, 1402, 93405]</td>\n",
       "      <td>[Game of Thrones, Game of Thrones, nan, Santo ...</td>\n",
       "      <td>[Il y a de l'orage dans l'air au royaume des S...</td>\n",
       "      <td>[Emilia Clarke; Peter Dinklage; Kit Harington;...</td>\n",
       "      <td>[David Nutter; Alan Taylor; Alex Graves, David...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 userId  \\\n",
       "3  c1d9a05e-50c1-7041-a60e-9a56c092e612   \n",
       "\n",
       "                                   favoriteGenresIds  \\\n",
       "3  [28, 10759, 12, 16, 10762, 80, 35, 99, 10764, ...   \n",
       "\n",
       "                                              Genres  \\\n",
       "3  [Acción, Action & Adventure, Aventura, Animaci...   \n",
       "\n",
       "                     favoriteMoviesIds  \\\n",
       "3  [27205, 157336, 155, 19995, 293660]   \n",
       "\n",
       "                                       Movies_Titles  \\\n",
       "3  [Perfect Match, Interstellar, Interstellar, Th...   \n",
       "\n",
       "                                     Movies_Synopsis  \\\n",
       "3  [Watch the latest Perfect Match (2018) Full on...   \n",
       "\n",
       "                                         Movies_Cast  \\\n",
       "3  [Feng Zhi Mo; Xu Ding, Matthew McConaughey; Je...   \n",
       "\n",
       "                                    Movies_Directors  \\\n",
       "3  [Ba Chen Xu, Christopher Nolan, Christopher No...   \n",
       "\n",
       "                   favoriteSeriesIds  \\\n",
       "3  [1399, 71446, 66732, 1402, 93405]   \n",
       "\n",
       "                                       Series_Titles  \\\n",
       "3  [Game of Thrones, Game of Thrones, nan, Santo ...   \n",
       "\n",
       "                                     Series_Synopsis  \\\n",
       "3  [Il y a de l'orage dans l'air au royaume des S...   \n",
       "\n",
       "                                         Series_Cast  \\\n",
       "3  [Emilia Clarke; Peter Dinklage; Kit Harington;...   \n",
       "\n",
       "                                    Series_Directors  \n",
       "3  [David Nutter; Alan Taylor; Alex Graves, David...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_pref[user_pref['userId']== 'c1d9a05e-50c1-7041-a60e-9a56c092e612'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences for vectorize from 'filtered_data' / AHORA DESDE FIREBASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the sentences to embed \n",
    "filtered_data['sentences_to_embed'] = (\n",
    "    filtered_data.CleanTitle.fillna('') +\n",
    "    filtered_data.Synopsis.fillna('') +\n",
    "    filtered_data.Genre.fillna('').apply(\n",
    "        lambda x: ', '.join(ast.literal_eval(x)) if x.startswith('[') and x.endswith(']') else x ) +\n",
    "    filtered_data.Cast.fillna('') +\n",
    "    filtered_data.Directors.fillna('')\n",
    ")\n",
    "\n",
    "ids_from_filtered_data = filtered_data['ID'].tolist()  # Guardamos los IDs\n",
    "sentences_from_filtered_data = filtered_data['sentences_to_embed'].dropna().astype(str).tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the model on Hugging Face processes only requests that can be completed within 60 seconds, we need to divide the sentences into batches.\n",
    "def split_into_batches(sentences, batch_size):\n",
    "    return [sentences[i:i + batch_size] for i in range(0, len(sentences), batch_size)]\n",
    "\n",
    "# After trying with different values, we've reach the maximum batch size to get response succesfully\n",
    "batches = split_into_batches(sentences_from_filtered_data, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clave de API cargada correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Check key availability\n",
    "if HUGGINGFACE_API_KEY is None:\n",
    "    print(\"Error: No se encontró la clave de API de Hugging Face.\")\n",
    "else:\n",
    "    print(\"Clave de API cargada correctamente.\")\n",
    "\n",
    "# Model URL\n",
    "API_URL = \"https://api-inference.huggingface.co/pipeline/feature-extraction/sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# API header and key\n",
    "headers = {\"Authorization\": f\"Bearer {HUGGINGFACE_API_KEY}\"}  \n",
    "\n",
    "# Function to get embeddings from Hugging Face API\n",
    "def get_embeddings_from_api(sentences):\n",
    "    url = API_URL\n",
    "    payload = {\"inputs\": sentences}\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=payload, timeout=10)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error {response.status_code}: {response.text}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "# all_embeddings_from_filtered_data = []\n",
    "# for batch in batches:\n",
    "#     print(f\"Processing batch with {len(batch)} sentences...\")\n",
    "#     time.sleep(7)\n",
    "#     embeddings = get_embeddings_from_api(batch)\n",
    "#     if embeddings:\n",
    "#         all_embeddings_from_filtered_data.extend(embeddings)\n",
    "\n",
    "# # Asociamos cada embedding con su respectivo ID\n",
    "# all_embeddings_dict = {id_: emb for id_, emb in zip(ids_from_filtered_data, all_embeddings_from_filtered_data)}\n",
    "\n",
    "# print(\"Embeddings processed successfully:\")\n",
    "# print(list(all_embeddings_dict.items())[:2])  # Muestra los primeros pares ID - embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences for vectorize from 'user_preferences' / AHORA DESDE DYNAMODB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################### DESDE ACA 13/02\n",
    "# Sentences we want to be embedded from user_preferences MOVIES\n",
    "user_pref['movies_sentences_to_embed'] = (user_pref.Movies_Titles.fillna('') +\n",
    "                                   user_pref.Movies_Synopsis.fillna('')+\n",
    "                                   user_pref.Genres.fillna('') +\n",
    "                                   user_pref.Movies_Cast.fillna('') +\n",
    "                                   user_pref.Movies_Directors.fillna(''))\n",
    "\n",
    "# Sentences we want to be embedded from user_preferences SERIES\n",
    "user_pref['series_sentences_to_embed'] = (user_pref.Series_Titles.fillna('') + \n",
    "                                   user_pref.Series_Synopsis.fillna('') +\n",
    "                                   user_pref.Genres.fillna('') +\n",
    "                                   user_pref.Series_Cast.fillna('') +\n",
    "                                   user_pref.Series_Directors.fillna(''))\n",
    "\n",
    "############################################################################################################## NUEVO BBBB\n",
    "# Guardar userId junto con la oración a vectorizar\n",
    "movies_sentences_from_user_pref = user_pref[['userId', 'movies_sentences_to_embed']].dropna().astype(str)\n",
    "series_sentences_from_user_pref = user_pref[['userId', 'series_sentences_to_embed']].dropna().astype(str)\n",
    "############################################################################################################## NUEVO BBBB CIERRO\n",
    "\n",
    "# We split the sentences in batches as we did previously with filtered_data\n",
    "movies_batches_user_pref = split_into_batches(movies_sentences_from_user_pref, 75)\n",
    "series_batches_user_pref = split_into_batches(series_sentences_from_user_pref, 75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing movie batch 1/124 with 75 sentences...\n",
      "Processing movie batch 2/124 with 75 sentences...\n",
      "Processing movie batch 3/124 with 75 sentences...\n",
      "Processing movie batch 4/124 with 75 sentences...\n",
      "Processing movie batch 5/124 with 75 sentences...\n",
      "Processing movie batch 6/124 with 75 sentences...\n",
      "Processing movie batch 7/124 with 75 sentences...\n",
      "Processing movie batch 8/124 with 75 sentences...\n",
      "Processing movie batch 9/124 with 75 sentences...\n",
      "Processing movie batch 10/124 with 75 sentences...\n",
      "Processing movie batch 11/124 with 75 sentences...\n",
      "Processing movie batch 12/124 with 75 sentences...\n",
      "Processing movie batch 13/124 with 75 sentences...\n",
      "Processing movie batch 14/124 with 75 sentences...\n",
      "Processing movie batch 15/124 with 75 sentences...\n",
      "Processing movie batch 16/124 with 75 sentences...\n",
      "Processing movie batch 17/124 with 75 sentences...\n",
      "Processing movie batch 18/124 with 75 sentences...\n",
      "Processing movie batch 19/124 with 75 sentences...\n",
      "Processing movie batch 20/124 with 75 sentences...\n",
      "Processing movie batch 21/124 with 75 sentences...\n",
      "Processing movie batch 22/124 with 75 sentences...\n",
      "Processing movie batch 23/124 with 75 sentences...\n",
      "Processing movie batch 24/124 with 75 sentences...\n",
      "Processing movie batch 25/124 with 75 sentences...\n",
      "Processing movie batch 26/124 with 75 sentences...\n",
      "Processing movie batch 27/124 with 75 sentences...\n",
      "Processing movie batch 28/124 with 75 sentences...\n",
      "Processing movie batch 29/124 with 75 sentences...\n",
      "Error 413: {\"error\":\"Payload Too Large\"}\n",
      "Processing movie batch 30/124 with 75 sentences...\n",
      "Processing movie batch 31/124 with 75 sentences...\n",
      "Processing movie batch 32/124 with 75 sentences...\n",
      "Processing movie batch 33/124 with 75 sentences...\n",
      "Processing movie batch 34/124 with 75 sentences...\n",
      "Processing movie batch 35/124 with 75 sentences...\n",
      "Error 413: {\"error\":\"Payload Too Large\"}\n",
      "Processing movie batch 36/124 with 75 sentences...\n",
      "Processing movie batch 37/124 with 75 sentences...\n",
      "Processing movie batch 38/124 with 75 sentences...\n",
      "Processing movie batch 39/124 with 75 sentences...\n",
      "Processing movie batch 40/124 with 75 sentences...\n",
      "Processing movie batch 41/124 with 75 sentences...\n",
      "Processing movie batch 42/124 with 75 sentences...\n",
      "Processing movie batch 43/124 with 75 sentences...\n",
      "Processing movie batch 44/124 with 75 sentences...\n",
      "Processing movie batch 45/124 with 75 sentences...\n",
      "Processing movie batch 46/124 with 75 sentences...\n",
      "Processing movie batch 47/124 with 75 sentences...\n",
      "Processing movie batch 48/124 with 75 sentences...\n",
      "Processing movie batch 49/124 with 75 sentences...\n",
      "Processing movie batch 50/124 with 75 sentences...\n",
      "Processing movie batch 51/124 with 75 sentences...\n",
      "Processing movie batch 52/124 with 75 sentences...\n",
      "Processing movie batch 53/124 with 75 sentences...\n",
      "Processing movie batch 54/124 with 75 sentences...\n",
      "Processing movie batch 55/124 with 75 sentences...\n",
      "Processing movie batch 56/124 with 75 sentences...\n",
      "Processing movie batch 57/124 with 75 sentences...\n",
      "Processing movie batch 58/124 with 75 sentences...\n",
      "Processing movie batch 59/124 with 75 sentences...\n",
      "Processing movie batch 60/124 with 75 sentences...\n",
      "Processing movie batch 61/124 with 75 sentences...\n",
      "Processing movie batch 62/124 with 75 sentences...\n",
      "Processing movie batch 63/124 with 75 sentences...\n",
      "Processing movie batch 64/124 with 75 sentences...\n",
      "Processing movie batch 65/124 with 75 sentences...\n",
      "Processing movie batch 66/124 with 75 sentences...\n",
      "Processing movie batch 67/124 with 75 sentences...\n",
      "Processing movie batch 68/124 with 75 sentences...\n",
      "Processing movie batch 69/124 with 75 sentences...\n",
      "Processing movie batch 70/124 with 75 sentences...\n",
      "Processing movie batch 71/124 with 75 sentences...\n",
      "Processing movie batch 72/124 with 75 sentences...\n",
      "Processing movie batch 73/124 with 75 sentences...\n",
      "Processing movie batch 74/124 with 75 sentences...\n",
      "Processing movie batch 75/124 with 75 sentences...\n",
      "Processing movie batch 76/124 with 75 sentences...\n",
      "Processing movie batch 77/124 with 75 sentences...\n",
      "Processing movie batch 78/124 with 75 sentences...\n",
      "Processing movie batch 79/124 with 75 sentences...\n",
      "Processing movie batch 80/124 with 75 sentences...\n",
      "Processing movie batch 81/124 with 75 sentences...\n",
      "Processing movie batch 82/124 with 75 sentences...\n",
      "Processing movie batch 83/124 with 75 sentences...\n",
      "Processing movie batch 84/124 with 75 sentences...\n",
      "Processing movie batch 85/124 with 75 sentences...\n",
      "Processing movie batch 86/124 with 75 sentences...\n",
      "Processing movie batch 87/124 with 75 sentences...\n",
      "Processing movie batch 88/124 with 75 sentences...\n",
      "Processing movie batch 89/124 with 75 sentences...\n",
      "Processing movie batch 90/124 with 75 sentences...\n",
      "Processing movie batch 91/124 with 75 sentences...\n",
      "Processing movie batch 92/124 with 75 sentences...\n",
      "Processing movie batch 93/124 with 75 sentences...\n",
      "Processing movie batch 94/124 with 75 sentences...\n",
      "Processing movie batch 95/124 with 75 sentences...\n",
      "Processing movie batch 96/124 with 75 sentences...\n",
      "Processing movie batch 97/124 with 75 sentences...\n",
      "Processing movie batch 98/124 with 75 sentences...\n",
      "Processing movie batch 99/124 with 75 sentences...\n",
      "Processing movie batch 100/124 with 75 sentences...\n",
      "Processing movie batch 101/124 with 75 sentences...\n",
      "Processing movie batch 102/124 with 75 sentences...\n",
      "Processing movie batch 103/124 with 75 sentences...\n",
      "Error 413: {\"error\":\"Payload Too Large\"}\n",
      "Processing movie batch 104/124 with 75 sentences...\n",
      "Processing movie batch 105/124 with 75 sentences...\n",
      "Processing movie batch 106/124 with 75 sentences...\n",
      "Error 413: {\"error\":\"Payload Too Large\"}\n",
      "Processing movie batch 107/124 with 75 sentences...\n",
      "Error 503: {\"error\":\"Model sentence-transformers/all-MiniLM-L6-v2 is currently loading\",\"estimated_time\":20.0}\n",
      "Processing movie batch 108/124 with 75 sentences...\n",
      "Error 503: {\"error\":\"Model sentence-transformers/all-MiniLM-L6-v2 is currently loading\",\"estimated_time\":20.0}\n",
      "Processing movie batch 109/124 with 75 sentences...\n",
      "Error 503: {\"error\":\"Model sentence-transformers/all-MiniLM-L6-v2 is currently loading\",\"estimated_time\":20.0}\n",
      "Processing movie batch 110/124 with 75 sentences...\n",
      "Error 413: {\"error\":\"Payload Too Large\"}\n",
      "Processing movie batch 111/124 with 75 sentences...\n",
      "Processing movie batch 112/124 with 75 sentences...\n",
      "Processing movie batch 113/124 with 75 sentences...\n",
      "Processing movie batch 114/124 with 75 sentences...\n",
      "Processing movie batch 115/124 with 75 sentences...\n",
      "Processing movie batch 116/124 with 75 sentences...\n",
      "Processing movie batch 117/124 with 75 sentences...\n",
      "Processing movie batch 118/124 with 75 sentences...\n",
      "Processing movie batch 119/124 with 75 sentences...\n",
      "Processing movie batch 120/124 with 75 sentences...\n",
      "Processing movie batch 121/124 with 75 sentences...\n",
      "Processing movie batch 122/124 with 75 sentences...\n",
      "Error 413: {\"error\":\"Payload Too Large\"}\n",
      "Processing movie batch 123/124 with 75 sentences...\n",
      "Error 413: {\"error\":\"Payload Too Large\"}\n",
      "Processing movie batch 124/124 with 59 sentences...\n",
      "Movies Embeddings processed successfully.\n"
     ]
    }
   ],
   "source": [
    "movies_embeddings_dict = {}  # Diccionario para almacenar {userId: embedding}\n",
    "num_batches = len(movies_batches_user_pref)  # Total de batches\n",
    "\n",
    "for i, batch in enumerate(movies_batches_user_pref, start=1):\n",
    "    time.sleep(1)\n",
    "    print(f\"Processing movie batch {i}/{num_batches} with {len(batch)} sentences...\")\n",
    "    \n",
    "    batch_user_ids = batch['userId'].tolist()\n",
    "    batch_sentences = batch['movies_sentences_to_embed'].tolist()\n",
    "    \n",
    "    embeddings = get_embeddings_from_api(batch_sentences)\n",
    "    if embeddings:\n",
    "        movies_embeddings_dict.update({uid: emb for uid, emb in zip(batch_user_ids, embeddings)})\n",
    "\n",
    "print(\"Movies Embeddings processed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing series batch 1/124 with 75 sentences...\n",
      "Processing series batch 2/124 with 75 sentences...\n",
      "Processing series batch 3/124 with 75 sentences...\n",
      "Processing series batch 4/124 with 75 sentences...\n",
      "Processing series batch 5/124 with 75 sentences...\n",
      "Processing series batch 6/124 with 75 sentences...\n",
      "Processing series batch 7/124 with 75 sentences...\n",
      "Processing series batch 8/124 with 75 sentences...\n",
      "Processing series batch 9/124 with 75 sentences...\n",
      "Processing series batch 10/124 with 75 sentences...\n",
      "Processing series batch 11/124 with 75 sentences...\n",
      "Processing series batch 12/124 with 75 sentences...\n",
      "Processing series batch 13/124 with 75 sentences...\n",
      "Processing series batch 14/124 with 75 sentences...\n",
      "Processing series batch 15/124 with 75 sentences...\n",
      "Processing series batch 16/124 with 75 sentences...\n",
      "Processing series batch 17/124 with 75 sentences...\n",
      "Processing series batch 18/124 with 75 sentences...\n",
      "Processing series batch 19/124 with 75 sentences...\n",
      "Processing series batch 20/124 with 75 sentences...\n",
      "Processing series batch 21/124 with 75 sentences...\n",
      "Processing series batch 22/124 with 75 sentences...\n",
      "Processing series batch 23/124 with 75 sentences...\n",
      "Processing series batch 24/124 with 75 sentences...\n",
      "Processing series batch 25/124 with 75 sentences...\n",
      "Processing series batch 26/124 with 75 sentences...\n",
      "Processing series batch 27/124 with 75 sentences...\n",
      "Processing series batch 28/124 with 75 sentences...\n",
      "Processing series batch 29/124 with 75 sentences...\n",
      "Processing series batch 30/124 with 75 sentences...\n",
      "Processing series batch 31/124 with 75 sentences...\n",
      "Processing series batch 32/124 with 75 sentences...\n",
      "Processing series batch 33/124 with 75 sentences...\n",
      "Processing series batch 34/124 with 75 sentences...\n",
      "Processing series batch 35/124 with 75 sentences...\n",
      "Processing series batch 36/124 with 75 sentences...\n",
      "Processing series batch 37/124 with 75 sentences...\n",
      "Processing series batch 38/124 with 75 sentences...\n",
      "Processing series batch 39/124 with 75 sentences...\n",
      "Processing series batch 40/124 with 75 sentences...\n",
      "Processing series batch 41/124 with 75 sentences...\n",
      "Processing series batch 42/124 with 75 sentences...\n",
      "Processing series batch 43/124 with 75 sentences...\n",
      "Processing series batch 44/124 with 75 sentences...\n",
      "Processing series batch 45/124 with 75 sentences...\n",
      "Processing series batch 46/124 with 75 sentences...\n",
      "Processing series batch 47/124 with 75 sentences...\n",
      "Processing series batch 48/124 with 75 sentences...\n",
      "Processing series batch 49/124 with 75 sentences...\n",
      "Processing series batch 50/124 with 75 sentences...\n",
      "Processing series batch 51/124 with 75 sentences...\n",
      "Processing series batch 52/124 with 75 sentences...\n",
      "Processing series batch 53/124 with 75 sentences...\n",
      "Processing series batch 54/124 with 75 sentences...\n",
      "Processing series batch 55/124 with 75 sentences...\n",
      "Processing series batch 56/124 with 75 sentences...\n",
      "Processing series batch 57/124 with 75 sentences...\n",
      "Processing series batch 58/124 with 75 sentences...\n",
      "Processing series batch 59/124 with 75 sentences...\n",
      "Processing series batch 60/124 with 75 sentences...\n",
      "Processing series batch 61/124 with 75 sentences...\n",
      "Processing series batch 62/124 with 75 sentences...\n",
      "Processing series batch 63/124 with 75 sentences...\n",
      "Processing series batch 64/124 with 75 sentences...\n",
      "Processing series batch 65/124 with 75 sentences...\n",
      "Processing series batch 66/124 with 75 sentences...\n",
      "Error 503: {\"error\":\"Model sentence-transformers/all-MiniLM-L6-v2 is currently loading\",\"estimated_time\":20.0}\n",
      "Processing series batch 67/124 with 75 sentences...\n",
      "Error 503: {\"error\":\"Model sentence-transformers/all-MiniLM-L6-v2 is currently loading\",\"estimated_time\":20.0}\n",
      "Processing series batch 68/124 with 75 sentences...\n",
      "Error 503: {\"error\":\"Model sentence-transformers/all-MiniLM-L6-v2 is currently loading\",\"estimated_time\":20.0}\n",
      "Processing series batch 69/124 with 75 sentences...\n",
      "Error 503: {\"error\":\"Model sentence-transformers/all-MiniLM-L6-v2 is currently loading\",\"estimated_time\":20.0}\n",
      "Processing series batch 70/124 with 75 sentences...\n",
      "Error 503: {\"error\":\"Model sentence-transformers/all-MiniLM-L6-v2 is currently loading\",\"estimated_time\":20.0}\n",
      "Processing series batch 71/124 with 75 sentences...\n",
      "Error 503: {\"error\":\"Service Unavailable\"}\n",
      "Processing series batch 72/124 with 75 sentences...\n",
      "Processing series batch 73/124 with 75 sentences...\n",
      "Processing series batch 74/124 with 75 sentences...\n",
      "Processing series batch 75/124 with 75 sentences...\n",
      "Processing series batch 76/124 with 75 sentences...\n",
      "Processing series batch 77/124 with 75 sentences...\n",
      "Processing series batch 78/124 with 75 sentences...\n",
      "Processing series batch 79/124 with 75 sentences...\n",
      "Processing series batch 80/124 with 75 sentences...\n",
      "Processing series batch 81/124 with 75 sentences...\n",
      "Processing series batch 82/124 with 75 sentences...\n",
      "Processing series batch 83/124 with 75 sentences...\n",
      "Processing series batch 84/124 with 75 sentences...\n",
      "Processing series batch 85/124 with 75 sentences...\n",
      "Processing series batch 86/124 with 75 sentences...\n",
      "Processing series batch 87/124 with 75 sentences...\n",
      "Processing series batch 88/124 with 75 sentences...\n",
      "Processing series batch 89/124 with 75 sentences...\n",
      "Processing series batch 90/124 with 75 sentences...\n",
      "Processing series batch 91/124 with 75 sentences...\n",
      "Processing series batch 92/124 with 75 sentences...\n",
      "Processing series batch 93/124 with 75 sentences...\n",
      "Processing series batch 94/124 with 75 sentences...\n",
      "Processing series batch 95/124 with 75 sentences...\n",
      "Processing series batch 96/124 with 75 sentences...\n",
      "Processing series batch 97/124 with 75 sentences...\n",
      "Processing series batch 98/124 with 75 sentences...\n",
      "Processing series batch 99/124 with 75 sentences...\n",
      "Processing series batch 100/124 with 75 sentences...\n",
      "Processing series batch 101/124 with 75 sentences...\n",
      "Processing series batch 102/124 with 75 sentences...\n",
      "Processing series batch 103/124 with 75 sentences...\n",
      "Processing series batch 104/124 with 75 sentences...\n",
      "Processing series batch 105/124 with 75 sentences...\n",
      "Processing series batch 106/124 with 75 sentences...\n",
      "Processing series batch 107/124 with 75 sentences...\n",
      "Processing series batch 108/124 with 75 sentences...\n",
      "Processing series batch 109/124 with 75 sentences...\n",
      "Processing series batch 110/124 with 75 sentences...\n",
      "Processing series batch 111/124 with 75 sentences...\n",
      "Processing series batch 112/124 with 75 sentences...\n",
      "Processing series batch 113/124 with 75 sentences...\n",
      "Processing series batch 114/124 with 75 sentences...\n",
      "Processing series batch 115/124 with 75 sentences...\n",
      "Processing series batch 116/124 with 75 sentences...\n",
      "Processing series batch 117/124 with 75 sentences...\n",
      "Processing series batch 118/124 with 75 sentences...\n",
      "Processing series batch 119/124 with 75 sentences...\n",
      "Processing series batch 120/124 with 75 sentences...\n",
      "Processing series batch 121/124 with 75 sentences...\n",
      "Processing series batch 122/124 with 75 sentences...\n",
      "Processing series batch 123/124 with 75 sentences...\n",
      "Processing series batch 124/124 with 59 sentences...\n",
      "Series Embeddings processed successfully.\n"
     ]
    }
   ],
   "source": [
    "series_embeddings_dict = {}  # Diccionario para almacenar {userId: embedding}\n",
    "num_batches_series = len(series_batches_user_pref)  # Total de batches\n",
    "\n",
    "for i, batch in enumerate(series_batches_user_pref, start=1):\n",
    "    time.sleep(1)\n",
    "    print(f\"Processing series batch {i}/{num_batches_series} with {len(batch)} sentences...\")\n",
    "    \n",
    "    batch_user_ids = batch['userId'].tolist()\n",
    "    batch_sentences = batch['series_sentences_to_embed'].tolist()\n",
    "    \n",
    "    embeddings = get_embeddings_from_api(batch_sentences)\n",
    "    if embeddings:\n",
    "        series_embeddings_dict.update({uid: emb for uid, emb in zip(batch_user_ids, embeddings)})\n",
    "\n",
    "print(\"Series Embeddings processed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movies Similarities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All this cell content is about movies\n",
    "# We have to convert the embeddings list to numpy arrays in order to calculate cosine similarities wiht sklearn\n",
    "movies_embeddings_from_user_pref_array = np.array(list(movies_embeddings_dict.values()))\n",
    "all_embeddings_from_filtered_data_array = np.array(list(all_embeddings_dict.values()))\n",
    "\n",
    "\n",
    "user_for_example = 2\n",
    "\n",
    "# Taking first user as example to calculate the cosine_similarity\n",
    "user_id_example = list(movies_embeddings_dict.keys())[user_for_example]\n",
    "movies_user_embedding_example = np.array(movies_embeddings_dict[user_id_example]).reshape(1, -1)  # Asegurar forma correcta para cosine_similarity\n",
    "\n",
    "\n",
    "# To calculate similarity between the user example embeding and the whole content from filtered data\n",
    "movies_content_similarities = cosine_similarity(movies_user_embedding_example, all_embeddings_from_filtered_data_array).flatten()\n",
    "\n",
    "# Sort indexes by similarity\n",
    "movies_most_similar_indexes = movies_content_similarities.argsort()[::-1]\n",
    "\n",
    "# Top-10\n",
    "movies_topten_most_similar_indexes = movies_most_similar_indexes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movies Most similar indexes: [132126 117993  88286  20613  21343  96796 126503  63667  17944  21200]\n",
      "Movies Highest similarities: [0.71462399 0.69705894 0.69705894 0.68328462 0.68118748 0.67463338\n",
      " 0.67463338 0.66778694 0.66632306 0.6490797 ]\n"
     ]
    }
   ],
   "source": [
    "# To display the movies most similar indexes and their similarity scores\n",
    "print(\"Movies Most similar indexes:\", movies_most_similar_indexes[:10])\n",
    "print(\"Movies Highest similarities:\", movies_content_similarities[movies_most_similar_indexes[:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tv Shows (Series) Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All this cell content is about series \n",
    "# We have to convert the embeddings list to numpy arrays in order to calculate cosine similarities wiht sklearn\n",
    "movies_user_embedding_example = np.array(movies_embeddings_dict[user_id_example]).reshape(1, -1)\n",
    "'''Estas lineas comentadas ya las hicen en movies, las dejo para verlas nada mas.\n",
    "all_embeddings_from_filtered_data_array = np.array(all_embeddings_from_filtered_data) esta linea ya la hice en movies\n",
    "\n",
    "user_for_example = 1'''\n",
    "\n",
    "# Taking first user as example to calculate the cosine_similarity\n",
    "series_user_embedding_example = np.array(series_embeddings_dict[user_id_example]).reshape(1, -1)  # forma correcta para cosine_similarity\n",
    "\n",
    "# To calculate similarity between the user example embeding and the whole content from filtered data\n",
    "series_content_similarities = cosine_similarity(series_user_embedding_example, all_embeddings_from_filtered_data_array).flatten()\n",
    "\n",
    "# Sort indexes by similarity\n",
    "series_most_similar_indexes = series_content_similarities.argsort()[::-1]\n",
    "\n",
    "# Top-10\n",
    "series_topten_most_similar_indexes = series_most_similar_indexes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To display the series most similar indexes and their similarity scores\n",
    "print(\"Series Most similar indexes:\", series_most_similar_indexes[:10])\n",
    "print(\"Series Highest similarities:\", series_content_similarities[series_most_similar_indexes[:10]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recomendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search most_similar_indexes, and preferences, and get recommendations\n",
    "user_id = user_id_example  \n",
    "movies_preferred = user_pref[user_pref['userId']==user_id]['Movies_Titles']\n",
    "series_preferred = user_pref[user_pref['userId']==user_id]['Series_Titles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series_preferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_pref[user_pref['userId']== 'e119804e-3011-700f-a5e3-e111aec24ac8'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search most_similar_indexes, and preferences, and get recommendations\n",
    "user_id = user_id_example  \n",
    "movies_preferred = user_pref[user_pref['userId']==user_id]['Movies_Titles']\n",
    "series_preferred = user_pref[user_pref['userId']==user_id]['Series_Titles']\n",
    "\n",
    "# Displaying Preferences & Recommendations\n",
    "print(f'''User {user_id} Preferences:\n",
    "      ''')\n",
    "print(f'''Movies preference:''')\n",
    "for movie in movies_preferred.iloc[0]:# .split(';'):\n",
    "    print(f'      {movie.strip()}')\n",
    "\n",
    "print(f'''TV Shows preference:''')\n",
    "for series in series_preferred.iloc[0]:# .split(';'):\n",
    "    print(f'''      {series.strip()}\n",
    "          ''')\n",
    "\n",
    "print(f'''Recomendations for user: \n",
    "      {user_id}\n",
    "      ''')\n",
    "\n",
    "################################################################################################################# NUEVO AAAA\n",
    "# Convertimos los índices más similares en IDs reales\n",
    "movies_recommended_ids = [filtered_data.iloc[i]['ID'] for i in movies_topten_most_similar_indexes]\n",
    "series_recommended_ids = [filtered_data.iloc[i]['ID'] for i in series_topten_most_similar_indexes]\n",
    "################################################################################################################# CIERRO NUEVO AAAA\n",
    "\n",
    "# Ahora buscamos los títulos usando los IDs reales\n",
    "movies_recomendations_user = filtered_data[filtered_data['ID'].isin(movies_recommended_ids)]['CleanTitle']\n",
    "series_recomendations_user = filtered_data[filtered_data['ID'].isin(series_recommended_ids)]['CleanTitle']\n",
    "\n",
    "\n",
    "print('Movies Recommendations:')\n",
    "for recommendation in movies_recomendations_user:\n",
    "    print(f'      {recommendation}')\n",
    "\n",
    "print('Tv Shows Recommendations:')\n",
    "for recommendation in series_recomendations_user:\n",
    "    print(f'      {recommendation}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
