{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import boto3\n",
    "import faiss\n",
    "import firebase_admin\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "from decimal import Decimal\n",
    "from dotenv import load_dotenv\n",
    "from firebase_admin import credentials, firestore\n",
    "from requests.exceptions import ReadTimeout\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "#from torch.nn.functional import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploading the environment variables and get the API key\n",
    "load_dotenv()\n",
    "HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "AWS_ACCESS_KEY = os.getenv(\"AWS_ACCESS_KEY\")\n",
    "AWS_SECRET_KEY = os.getenv(\"AWS_SECRET_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genres database, local by now. Then we have to get them linked to firebase, or wathever\n",
    "df_genres = pd.read_csv(r'generos.csv', sep=',')\n",
    "\n",
    "all_embeddings_from_filtered_data = pd.read_csv(r'../../../all_content_embeddings.csv')\n",
    "# filtered_data = pd.read_csv(r'../../../Test_clean.csv') #Queda esto comentado porque estoy trayendo desde firebase por los duplicados y triplicados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Conversi√≥n optimizada para el dict de los embeddings que habiamos guardado en csv\n",
    "def fast_convert(emb):\n",
    "    if isinstance(emb, str): \n",
    "        return np.array(json.loads(emb), dtype=np.float32)  # Usa float32 para ahorrar memoria\n",
    "    return emb\n",
    "\n",
    "all_embeddings_dict = {\n",
    "    id_: fast_convert(emb)\n",
    "    for id_, emb in zip(\n",
    "        all_embeddings_from_filtered_data['ID'], \n",
    "        all_embeddings_from_filtered_data['Embedding']\n",
    "    )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Data from FireBase (remains missing the conextion to firebase collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Firebase if it's not done (I KEEP THIS FOR THE MOMENT THE DICT OF EMBEDDIGNS WOULD BE A COLLECTION OF)\n",
    "if not firebase_admin._apps:\n",
    "    cred_path = r'../../../bubbo-dfba0-firebase-adminsdk-fbsvc-79dc4511e7.json'  \n",
    "    cred = credentials.Certificate(cred_path)\n",
    "    firebase_admin.initialize_app(cred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta celda por el momento tampoco sirve si se obtienen los datos local\n",
    "\n",
    "# Firestore conexion and db collection name\n",
    "db = firestore.client()\n",
    "collection_ref = db.collection('Data_Clean') # Look for the new collection\n",
    "# to get it all\n",
    "docs = collection_ref.stream()\n",
    "# documents to dictionaries\n",
    "data = [{**doc.to_dict(), 'id': doc.id} for doc in docs]\n",
    "df = pd.DataFrame(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 141207 entries, 0 to 141441\n",
      "Data columns (total 9 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   CleanTitle    141207 non-null  object\n",
      " 1   PlatformName  141207 non-null  object\n",
      " 2   ID            141207 non-null  object\n",
      " 3   Genre         141207 non-null  object\n",
      " 4   Type          141207 non-null  object\n",
      " 5   Synopsis      141207 non-null  object\n",
      " 6   Cast          141207 non-null  object\n",
      " 7   Directors     141207 non-null  object\n",
      " 8   id            141207 non-null  object\n",
      "dtypes: object(9)\n",
      "memory usage: 10.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# Cuando corro esta celda, \"reseteo\" a filtered_data\n",
    "filtered_data = df # Comentada porque ahora la saco de local ya que los embeddings ya estan \n",
    "filtered_data = filtered_data.replace(\"\",pd.NA)\n",
    "filtered_data = filtered_data.dropna()                                                                \n",
    "filtered_data = filtered_data.drop_duplicates()\n",
    "filtered_data['ID'] = filtered_data['ID'].astype(str)\n",
    "filtered_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Preferences from DynamoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the info from DynamoDB, user preferences\n",
    "CONFIG = {\n",
    "    'aws': {\n",
    "        'access_key': AWS_ACCESS_KEY,\n",
    "        'secret_key': AWS_SECRET_KEY,\n",
    "        'region': 'eu-west-3',\n",
    "        'table': 'User-7kkcm5dn2rb77hst5nh7gbdisa-staging'\n",
    "    },\n",
    "    'columns': ['userId', 'favoriteMoviesIds', 'favoriteGenresIds', 'favoriteSeriesIds'],\n",
    "}\n",
    "\n",
    "# conexion\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=CONFIG['aws']['access_key'],\n",
    "    aws_secret_access_key=CONFIG['aws']['secret_key'],\n",
    "    region_name=CONFIG['aws']['region']\n",
    ")\n",
    "\n",
    "table = session.resource('dynamodb').Table(CONFIG['aws']['table'])\n",
    "\n",
    "# Values to String\n",
    "def _process_value(value):\n",
    "    if isinstance(value, Decimal):\n",
    "        return str(int(value))\n",
    "    return str(value)\n",
    "\n",
    "# Retrive info from DynamoDB and gets a DataFrame\n",
    "def fetch_preferences():\n",
    "    try:\n",
    "        items = []\n",
    "        start_key = None\n",
    "\n",
    "        while True:\n",
    "            # scan with defined 'columns'  in previous 'CONFIG'\n",
    "            scan_params = {\n",
    "                'ProjectionExpression': ', '.join(CONFIG['columns'])\n",
    "            }\n",
    "            if start_key:\n",
    "                scan_params['ExclusiveStartKey'] = start_key\n",
    "\n",
    "            response = table.scan(**scan_params)\n",
    "            items.extend(response.get('Items', []))\n",
    "\n",
    "            # check for next pages\n",
    "            start_key = response.get('LastEvaluatedKey')\n",
    "            if not start_key:\n",
    "                break\n",
    "\n",
    "        # data extracted processing\n",
    "        processed_data = [{\n",
    "            'userId': _process_value(item.get('userId', '')),\n",
    "            'favoriteMoviesIds': ';'.join(map(_process_value, item.get('favoriteMoviesIds', []))),     ###################### DIRECTOR MAS CAST HAY QUE TRAER DESPUES CUANDO COMPLETO EL DF LUEGO DE FILTERED_DATA\n",
    "            'favoriteGenresIds': ';'.join(map(_process_value, item.get('favoriteGenresIds', []))),\n",
    "            'favoriteSeriesIds': ';'.join(map(_process_value, item.get('favoriteSeriesIds', [])))\n",
    "        } for item in items]\n",
    "\n",
    "        df = pd.DataFrame(processed_data)\n",
    "        return df\n",
    "\n",
    "    except ClientError as e:\n",
    "        print(f\"Error al conectar con DynamoDB: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# calling function to get the df\n",
    "user_pref = fetch_preferences()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates: 0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9305 entries, 0 to 9304\n",
      "Data columns (total 4 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   userId             9305 non-null   object\n",
      " 1   favoriteMoviesIds  9305 non-null   object\n",
      " 2   favoriteGenresIds  9305 non-null   object\n",
      " 3   favoriteSeriesIds  9305 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 290.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# limpio el dataframe dejando solo users con genero, movie_favs y tvshow_favs\n",
    "user_pref = user_pref[user_pref['userId'].str.len()==36]\n",
    "user_pref = user_pref.replace(\"\",pd.NA)\n",
    "user_pref = user_pref.dropna()             \n",
    "user_pref.reset_index(inplace=True,drop=True)\n",
    "print(f'Duplicates: {user_pref.duplicated().sum()}')\n",
    "user_pref.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>favoriteMoviesIds</th>\n",
       "      <th>favoriteGenresIds</th>\n",
       "      <th>favoriteSeriesIds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>a189607e-3041-70a5-3daf-01cd2118baff</td>\n",
       "      <td>671;672;673;135397;354912</td>\n",
       "      <td>10749;27;9648;18;10770;10766;10767;53;878;1076...</td>\n",
       "      <td>66732;93405;18165;119051;65334</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   userId          favoriteMoviesIds  \\\n",
       "103  a189607e-3041-70a5-3daf-01cd2118baff  671;672;673;135397;354912   \n",
       "\n",
       "                                     favoriteGenresIds  \\\n",
       "103  10749;27;9648;18;10770;10766;10767;53;878;1076...   \n",
       "\n",
       "                  favoriteSeriesIds  \n",
       "103  66732;93405;18165;119051;65334  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_pref[user_pref['userId']== 'a189607e-3041-70a5-3daf-01cd2118baff'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora tengo que traer tambien las Cast y Director\n",
    "\n",
    "# convertir los valores en listas para expandirlos con explode\n",
    "user_pref['favoriteGenresIds'] = user_pref['favoriteGenresIds'].apply(lambda x: x.split(';'))\n",
    "user_pref['favoriteMoviesIds'] = user_pref['favoriteMoviesIds'].apply(lambda x: x.split(';'))\n",
    "user_pref['favoriteSeriesIds'] = user_pref['favoriteSeriesIds'].apply(lambda x: x.split(';'))\n",
    "\n",
    "# expandir preferencias de favoriteMoviesIds, favoriteGenresIds, y favoriteSeriesIds por userId\n",
    "user_fav_genres = user_pref[['userId','favoriteGenresIds']].explode('favoriteGenresIds')\n",
    "user_fav_movies = user_pref[['userId','favoriteMoviesIds']].explode('favoriteMoviesIds')\n",
    "user_fav_series = user_pref[['userId','favoriteSeriesIds']].explode('favoriteSeriesIds')\n",
    "\n",
    "\n",
    "# merge para traerme los CleanTitle, Synopsis, 'Genre'\n",
    "user_fav_genres['favoriteGenresIds'] = user_fav_genres['favoriteGenresIds'].astype(int)\n",
    "user_fav_genres = user_fav_genres.merge(df_genres[['genero_id','genero_name']], left_on='favoriteGenresIds', right_on='genero_id') \n",
    "\n",
    "\n",
    "filtered_data = filtered_data.dropna(subset=['ID'])                                                                                    # <- Nuevo agregado# <- Nuevo agregado# <- Nuevo agregado\n",
    "filtered_data['ID'] = filtered_data['ID'].astype(str).str.strip()                                                                                 # <- Nuevo agregado# <- Nuevo agregado# <- Nuevo agregado\n",
    "user_fav_movies['favoriteMoviesIds'] = user_fav_movies['favoriteMoviesIds'].astype(str).str.strip()\n",
    "user_fav_series['favoriteSeriesIds'] = user_fav_series['favoriteSeriesIds'].astype(str).str.strip()\n",
    "\n",
    "user_fav_movies = user_fav_movies.merge(filtered_data[['ID','CleanTitle','Synopsis', 'Cast', 'Directors']], left_on='favoriteMoviesIds', right_on='ID', how='left')  ###### en esta y lasig fila agregu√© synopsis\n",
    "user_fav_series = user_fav_series.merge(filtered_data[['ID','CleanTitle','Synopsis', 'Cast', 'Directors']], left_on='favoriteSeriesIds', right_on='ID', how='left')\n",
    "\n",
    "user_fav_genres = user_fav_genres.drop(columns='genero_id')\n",
    "user_fav_genres.rename(columns={'genero_name':'Genres'}, inplace=True)\n",
    "user_fav_movies = user_fav_movies.drop(columns='ID')\n",
    "user_fav_movies.rename(columns={'CleanTitle':'Movies_Titles', 'Synopsis':'Movies_Synopsis', 'Cast':'Movies_Cast', 'Directors':'Movies_Directors'}, inplace=True)\n",
    "user_fav_series = user_fav_series.drop(columns='ID')\n",
    "user_fav_series.rename(columns={'CleanTitle':'Series_Titles', 'Synopsis':'Series_Synopsis', 'Cast':'Series_Cast', 'Directors':'Series_Directors'}, inplace=True)\n",
    "\n",
    "# reAGRUPO por userId para que me queden las listas CleanTitle, Synopsis, 'Genre' por user segun sus favoriteMoviesIds, favoriteGenresIds, y favoriteSeriesIds por userId\n",
    "user_fav_genres = user_fav_genres.groupby('userId')[['favoriteGenresIds','Genres']].agg(list).reset_index()\n",
    "user_fav_movies = user_fav_movies.groupby('userId')[['favoriteMoviesIds','Movies_Titles', 'Movies_Synopsis', 'Movies_Cast', 'Movies_Directors']].agg(list).reset_index()\n",
    "user_fav_series = user_fav_series.groupby('userId')[['favoriteSeriesIds','Series_Titles', 'Series_Synopsis', 'Series_Cast', 'Series_Directors']].agg(list).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>favoriteMoviesIds</th>\n",
       "      <th>favoriteGenresIds</th>\n",
       "      <th>favoriteSeriesIds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>a189607e-3041-70a5-3daf-01cd2118baff</td>\n",
       "      <td>[671, 672, 673, 135397, 354912]</td>\n",
       "      <td>[10749, 27, 9648, 18, 10770, 10766, 10767, 53,...</td>\n",
       "      <td>[66732, 93405, 18165, 119051, 65334]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   userId                favoriteMoviesIds  \\\n",
       "103  a189607e-3041-70a5-3daf-01cd2118baff  [671, 672, 673, 135397, 354912]   \n",
       "\n",
       "                                     favoriteGenresIds  \\\n",
       "103  [10749, 27, 9648, 18, 10770, 10766, 10767, 53,...   \n",
       "\n",
       "                        favoriteSeriesIds  \n",
       "103  [66732, 93405, 18165, 119051, 65334]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_pref[user_pref['userId']== 'a189607e-3041-70a5-3daf-01cd2118baff'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#termino de acomodar 'user_pref' para dar paso a los embeddings\n",
    "user_pref = user_pref.merge(user_fav_genres, left_on='userId', right_on='userId').drop(columns=['favoriteGenresIds_y'])\n",
    "user_pref.rename(columns={'favoriteGenresIds_x':'favoriteGenresIds'},inplace=True)\n",
    "user_pref = user_pref.merge(user_fav_movies, left_on='userId', right_on='userId').drop(columns=['favoriteMoviesIds_y'])\n",
    "user_pref.rename(columns={'favoriteMoviesIds_x':'favoriteMoviesIds'},inplace=True)\n",
    "user_pref = user_pref.merge(user_fav_series, left_on='userId', right_on='userId').drop(columns=['favoriteSeriesIds_y'])\n",
    "user_pref.rename(columns={'favoriteSeriesIds_x':'favoriteSeriesIds'},inplace=True)\n",
    "user_pref = user_pref.reindex(['userId', 'favoriteGenresIds', 'Genres', 'favoriteMoviesIds', 'Movies_Titles','Movies_Synopsis', 'Movies_Cast', 'Movies_Directors', 'favoriteSeriesIds', 'Series_Titles', 'Series_Synopsis', 'Series_Cast', 'Series_Directors'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>favoriteGenresIds</th>\n",
       "      <th>Genres</th>\n",
       "      <th>favoriteMoviesIds</th>\n",
       "      <th>Movies_Titles</th>\n",
       "      <th>Movies_Synopsis</th>\n",
       "      <th>Movies_Cast</th>\n",
       "      <th>Movies_Directors</th>\n",
       "      <th>favoriteSeriesIds</th>\n",
       "      <th>Series_Titles</th>\n",
       "      <th>Series_Synopsis</th>\n",
       "      <th>Series_Cast</th>\n",
       "      <th>Series_Directors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c1d9a05e-50c1-7041-a60e-9a56c092e612</td>\n",
       "      <td>[28, 10759, 12, 16, 10762, 80, 35, 99, 10764, ...</td>\n",
       "      <td>[Acci√≥n, Action &amp; Adventure, Aventura, Animaci...</td>\n",
       "      <td>[27205, 157336, 155, 19995, 293660]</td>\n",
       "      <td>[Inception, Interstellar, The Dark Knight, Ava...</td>\n",
       "      <td>[Dom Cobb (Leonardo DiCaprio) is a skilled thi...</td>\n",
       "      <td>[Leonardo Dicaprio; Joseph Gordonlevitt; Ellio...</td>\n",
       "      <td>[Christopher Nolan, Christopher Nolan, Christo...</td>\n",
       "      <td>[1399, 71446, 66732, 1402, 93405]</td>\n",
       "      <td>[Game of Thrones, Lord, All Men Can't Be Dogs,...</td>\n",
       "      <td>[Il y a de l'orage dans l'air au royaume des S...</td>\n",
       "      <td>[Emilia Clarke; Peter Dinklage; Kit Harington;...</td>\n",
       "      <td>[David Nutter; Alan Taylor; Alex Graves, T.J. ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 userId  \\\n",
       "3  c1d9a05e-50c1-7041-a60e-9a56c092e612   \n",
       "\n",
       "                                   favoriteGenresIds  \\\n",
       "3  [28, 10759, 12, 16, 10762, 80, 35, 99, 10764, ...   \n",
       "\n",
       "                                              Genres  \\\n",
       "3  [Acci√≥n, Action & Adventure, Aventura, Animaci...   \n",
       "\n",
       "                     favoriteMoviesIds  \\\n",
       "3  [27205, 157336, 155, 19995, 293660]   \n",
       "\n",
       "                                       Movies_Titles  \\\n",
       "3  [Inception, Interstellar, The Dark Knight, Ava...   \n",
       "\n",
       "                                     Movies_Synopsis  \\\n",
       "3  [Dom Cobb (Leonardo DiCaprio) is a skilled thi...   \n",
       "\n",
       "                                         Movies_Cast  \\\n",
       "3  [Leonardo Dicaprio; Joseph Gordonlevitt; Ellio...   \n",
       "\n",
       "                                    Movies_Directors  \\\n",
       "3  [Christopher Nolan, Christopher Nolan, Christo...   \n",
       "\n",
       "                   favoriteSeriesIds  \\\n",
       "3  [1399, 71446, 66732, 1402, 93405]   \n",
       "\n",
       "                                       Series_Titles  \\\n",
       "3  [Game of Thrones, Lord, All Men Can't Be Dogs,...   \n",
       "\n",
       "                                     Series_Synopsis  \\\n",
       "3  [Il y a de l'orage dans l'air au royaume des S...   \n",
       "\n",
       "                                         Series_Cast  \\\n",
       "3  [Emilia Clarke; Peter Dinklage; Kit Harington;...   \n",
       "\n",
       "                                    Series_Directors  \n",
       "3  [David Nutter; Alan Taylor; Alex Graves, T.J. ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_pref[user_pref['userId']== 'c1d9a05e-50c1-7041-a60e-9a56c092e612'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences for vectorize from 'filtered_data' / AHORA DESDE FIREBASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the sentences to embed \n",
    "filtered_data['sentences_to_embed'] = (\n",
    "    filtered_data.CleanTitle.fillna('') +\n",
    "    filtered_data.Synopsis.fillna('') +\n",
    "    filtered_data.Genre.fillna('').apply(\n",
    "        lambda x: ', '.join(ast.literal_eval(x)) if x.startswith('[') and x.endswith(']') else x ) +\n",
    "    filtered_data.Cast.fillna('') +\n",
    "    filtered_data.Directors.fillna('')\n",
    ")\n",
    "\n",
    "ids_from_filtered_data = filtered_data['ID'].tolist()  # Guardamos los IDs\n",
    "sentences_from_filtered_data = filtered_data['sentences_to_embed'].dropna().astype(str).tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the model on Hugging Face processes only requests that can be completed within 60 seconds, we need to divide the sentences into batches.\n",
    "def split_into_batches(sentences, batch_size):\n",
    "    return [sentences[i:i + batch_size] for i in range(0, len(sentences), batch_size)]\n",
    "\n",
    "# After trying with different values, we've reach the maximum batch size to get response succesfully\n",
    "batches = split_into_batches(sentences_from_filtered_data, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clave de API cargada correctamente.\n"
     ]
    }
   ],
   "source": [
    "# Check key availability\n",
    "if HUGGINGFACE_API_KEY is None:\n",
    "    print(\"Error: No se encontr√≥ la clave de API de Hugging Face.\")\n",
    "else:\n",
    "    print(\"Clave de API cargada correctamente.\")\n",
    "\n",
    "# Model URL\n",
    "API_URL = \"https://api-inference.huggingface.co/pipeline/feature-extraction/sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# API header and key\n",
    "headers = {\"Authorization\": f\"Bearer {HUGGINGFACE_API_KEY}\"}  \n",
    "\n",
    "# Function to get embeddings from Hugging Face API\n",
    "def get_embeddings_from_api(sentences):\n",
    "    url = API_URL\n",
    "    payload = {\"inputs\": sentences}\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=payload, timeout=10)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error {response.status_code}: {response.text}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "# all_embeddings_from_filtered_data = []\n",
    "# for batch in batches:\n",
    "#     print(f\"Processing batch with {len(batch)} sentences...\")\n",
    "#     time.sleep(7)\n",
    "#     embeddings = get_embeddings_from_api(batch)\n",
    "#     if embeddings:\n",
    "#         all_embeddings_from_filtered_data.extend(embeddings)\n",
    "\n",
    "# # Asociamos cada embedding con su respectivo ID\n",
    "# all_embeddings_dict = {id_: emb for id_, emb in zip(ids_from_filtered_data, all_embeddings_from_filtered_data)}\n",
    "\n",
    "# print(\"Embeddings processed successfully:\")\n",
    "# print(list(all_embeddings_dict.items())[:2])  # Muestra los primeros pares ID - embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences for vectorize from 'user_preferences' / AHORA DESDE DYNAMODB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################################################################### DESDE ACA 13/02\n",
    "# Sentences we want to be embedded from user_preferences MOVIES\n",
    "user_pref['movies_sentences_to_embed'] = (user_pref.Movies_Titles.fillna('') +\n",
    "                                   user_pref.Movies_Synopsis.fillna('')+\n",
    "                                   user_pref.Genres.fillna('') +\n",
    "                                   user_pref.Movies_Cast.fillna('') +\n",
    "                                   user_pref.Movies_Directors.fillna(''))\n",
    "\n",
    "# Sentences we want to be embedded from user_preferences SERIES\n",
    "user_pref['series_sentences_to_embed'] = (user_pref.Series_Titles.fillna('') + \n",
    "                                   user_pref.Series_Synopsis.fillna('') +\n",
    "                                   user_pref.Genres.fillna('') +\n",
    "                                   user_pref.Series_Cast.fillna('') +\n",
    "                                   user_pref.Series_Directors.fillna(''))\n",
    "\n",
    "############################################################################################################## NUEVO BBBB\n",
    "# Guardar userId junto con la oraci√≥n a vectorizar\n",
    "movies_sentences_from_user_pref = user_pref[['userId', 'movies_sentences_to_embed']].dropna().astype(str)\n",
    "series_sentences_from_user_pref = user_pref[['userId', 'series_sentences_to_embed']].dropna().astype(str)\n",
    "############################################################################################################## NUEVO BBBB CIERRO\n",
    "\n",
    "# We split the sentences in batches as we did previously with filtered_data\n",
    "movies_batches_user_pref = split_into_batches(movies_sentences_from_user_pref, 50)\n",
    "series_batches_user_pref = split_into_batches(series_sentences_from_user_pref, 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing movies batch 1/187 with 50 sentences...\n",
      "Processing movies batch 2/187 with 50 sentences...\n",
      "Processing movies batch 3/187 with 50 sentences...\n",
      "Processing movies batch 4/187 with 50 sentences...\n",
      "Processing movies batch 5/187 with 50 sentences...\n",
      "Processing movies batch 6/187 with 50 sentences...\n",
      "Processing movies batch 7/187 with 50 sentences...\n",
      "Processing movies batch 8/187 with 50 sentences...\n",
      "Processing movies batch 9/187 with 50 sentences...\n",
      "Processing movies batch 10/187 with 50 sentences...\n",
      "Processing movies batch 11/187 with 50 sentences...\n",
      "Processing movies batch 12/187 with 50 sentences...\n",
      "Processing movies batch 13/187 with 50 sentences...\n",
      "Processing movies batch 14/187 with 50 sentences...\n",
      "Processing movies batch 15/187 with 50 sentences...\n",
      "Processing movies batch 16/187 with 50 sentences...\n",
      "Processing movies batch 17/187 with 50 sentences...\n",
      "Processing movies batch 18/187 with 50 sentences...\n",
      "Processing movies batch 19/187 with 50 sentences...\n",
      "Processing movies batch 20/187 with 50 sentences...\n",
      "Processing movies batch 21/187 with 50 sentences...\n",
      "Processing movies batch 22/187 with 50 sentences...\n",
      "Processing movies batch 23/187 with 50 sentences...\n",
      "Processing movies batch 24/187 with 50 sentences...\n",
      "Processing movies batch 25/187 with 50 sentences...\n",
      "Processing movies batch 26/187 with 50 sentences...\n",
      "Processing movies batch 27/187 with 50 sentences...\n",
      "Processing movies batch 28/187 with 50 sentences...\n",
      "Processing movies batch 29/187 with 50 sentences...\n",
      "Processing movies batch 30/187 with 50 sentences...\n",
      "Processing movies batch 31/187 with 50 sentences...\n",
      "Processing movies batch 32/187 with 50 sentences...\n",
      "Processing movies batch 33/187 with 50 sentences...\n",
      "Processing movies batch 34/187 with 50 sentences...\n",
      "Processing movies batch 35/187 with 50 sentences...\n",
      "Processing movies batch 36/187 with 50 sentences...\n",
      "Processing movies batch 37/187 with 50 sentences...\n",
      "Processing movies batch 38/187 with 50 sentences...\n",
      "Processing movies batch 39/187 with 50 sentences...\n",
      "Processing movies batch 40/187 with 50 sentences...\n",
      "Processing movies batch 41/187 with 50 sentences...\n",
      "Processing movies batch 42/187 with 50 sentences...\n",
      "Processing movies batch 43/187 with 50 sentences...\n",
      "Processing movies batch 44/187 with 50 sentences...\n",
      "Processing movies batch 45/187 with 50 sentences...\n",
      "Processing movies batch 46/187 with 50 sentences...\n",
      "Processing movies batch 47/187 with 50 sentences...\n",
      "Processing movies batch 48/187 with 50 sentences...\n",
      "Processing movies batch 49/187 with 50 sentences...\n",
      "Processing movies batch 50/187 with 50 sentences...\n",
      "Error 503: {\"error\":\"Model sentence-transformers/all-MiniLM-L6-v2 is currently loading\",\"estimated_time\":20.0}\n",
      "Processing movies batch 50/187 with 50 sentences...\n",
      "Error 503: {\"error\":\"Model sentence-transformers/all-MiniLM-L6-v2 is currently loading\",\"estimated_time\":20.0}\n",
      "Processing movies batch 50/187 with 50 sentences...\n",
      "Error 503: {\"error\":\"Model sentence-transformers/all-MiniLM-L6-v2 is currently loading\",\"estimated_time\":20.0}\n",
      "Processing movies batch 50/187 with 50 sentences...\n",
      "Error 503: {\"error\":\"Model sentence-transformers/all-MiniLM-L6-v2 is currently loading\",\"estimated_time\":20.0}\n",
      "Processing movies batch 50/187 with 50 sentences...\n",
      "Error 503: {\"error\":\"Model sentence-transformers/all-MiniLM-L6-v2 is currently loading\",\"estimated_time\":20.0}\n",
      "Processing movies batch 50/187 with 50 sentences...\n",
      "Error 503: {\"error\":\"Model sentence-transformers/all-MiniLM-L6-v2 is currently loading\",\"estimated_time\":20.0}\n",
      "Processing movies batch 50/187 with 50 sentences...\n",
      "Processing movies batch 51/187 with 50 sentences...\n",
      "Processing movies batch 52/187 with 50 sentences...\n",
      "Processing movies batch 53/187 with 50 sentences...\n",
      "Processing movies batch 54/187 with 50 sentences...\n",
      "Processing movies batch 55/187 with 50 sentences...\n",
      "Processing movies batch 56/187 with 50 sentences...\n",
      "Processing movies batch 57/187 with 50 sentences...\n",
      "Processing movies batch 58/187 with 50 sentences...\n",
      "Processing movies batch 59/187 with 50 sentences...\n",
      "Processing movies batch 60/187 with 50 sentences...\n",
      "Processing movies batch 61/187 with 50 sentences...\n",
      "Processing movies batch 62/187 with 50 sentences...\n",
      "Processing movies batch 63/187 with 50 sentences...\n",
      "Processing movies batch 64/187 with 50 sentences...\n",
      "Processing movies batch 65/187 with 50 sentences...\n",
      "Processing movies batch 66/187 with 50 sentences...\n",
      "Processing movies batch 67/187 with 50 sentences...\n",
      "Processing movies batch 68/187 with 50 sentences...\n",
      "Processing movies batch 69/187 with 50 sentences...\n",
      "Processing movies batch 70/187 with 50 sentences...\n",
      "Processing movies batch 71/187 with 50 sentences...\n",
      "Processing movies batch 72/187 with 50 sentences...\n",
      "Processing movies batch 73/187 with 50 sentences...\n",
      "Processing movies batch 74/187 with 50 sentences...\n",
      "Processing movies batch 75/187 with 50 sentences...\n",
      "Processing movies batch 76/187 with 50 sentences...\n",
      "Processing movies batch 77/187 with 50 sentences...\n",
      "Processing movies batch 78/187 with 50 sentences...\n",
      "Processing movies batch 79/187 with 50 sentences...\n",
      "Processing movies batch 80/187 with 50 sentences...\n",
      "Processing movies batch 81/187 with 50 sentences...\n",
      "Processing movies batch 82/187 with 50 sentences...\n",
      "Processing movies batch 83/187 with 50 sentences...\n",
      "Processing movies batch 84/187 with 50 sentences...\n",
      "Processing movies batch 85/187 with 50 sentences...\n",
      "Processing movies batch 86/187 with 50 sentences...\n",
      "Processing movies batch 87/187 with 50 sentences...\n",
      "Processing movies batch 88/187 with 50 sentences...\n",
      "Processing movies batch 89/187 with 50 sentences...\n",
      "Processing movies batch 90/187 with 50 sentences...\n",
      "Processing movies batch 91/187 with 50 sentences...\n",
      "Processing movies batch 92/187 with 50 sentences...\n",
      "Processing movies batch 93/187 with 50 sentences...\n",
      "Processing movies batch 94/187 with 50 sentences...\n",
      "Processing movies batch 95/187 with 50 sentences...\n",
      "Processing movies batch 96/187 with 50 sentences...\n",
      "Processing movies batch 97/187 with 50 sentences...\n",
      "Processing movies batch 98/187 with 50 sentences...\n",
      "Processing movies batch 99/187 with 50 sentences...\n",
      "Processing movies batch 100/187 with 50 sentences...\n",
      "Processing movies batch 101/187 with 50 sentences...\n",
      "Processing movies batch 102/187 with 50 sentences...\n",
      "Processing movies batch 103/187 with 50 sentences...\n",
      "Processing movies batch 104/187 with 50 sentences...\n",
      "Processing movies batch 105/187 with 50 sentences...\n",
      "Processing movies batch 106/187 with 50 sentences...\n",
      "Processing movies batch 107/187 with 50 sentences...\n",
      "Processing movies batch 108/187 with 50 sentences...\n",
      "Processing movies batch 109/187 with 50 sentences...\n",
      "Processing movies batch 110/187 with 50 sentences...\n",
      "Processing movies batch 111/187 with 50 sentences...\n",
      "Processing movies batch 112/187 with 50 sentences...\n",
      "Processing movies batch 113/187 with 50 sentences...\n",
      "Processing movies batch 114/187 with 50 sentences...\n",
      "Processing movies batch 115/187 with 50 sentences...\n",
      "Processing movies batch 116/187 with 50 sentences...\n",
      "Processing movies batch 117/187 with 50 sentences...\n",
      "Processing movies batch 118/187 with 50 sentences...\n",
      "Processing movies batch 119/187 with 50 sentences...\n",
      "Processing movies batch 120/187 with 50 sentences...\n",
      "Processing movies batch 121/187 with 50 sentences...\n",
      "Processing movies batch 122/187 with 50 sentences...\n",
      "Processing movies batch 123/187 with 50 sentences...\n",
      "Processing movies batch 124/187 with 50 sentences...\n",
      "Processing movies batch 125/187 with 50 sentences...\n",
      "Processing movies batch 126/187 with 50 sentences...\n",
      "Processing movies batch 127/187 with 50 sentences...\n",
      "Processing movies batch 128/187 with 50 sentences...\n",
      "Processing movies batch 129/187 with 50 sentences...\n",
      "Processing movies batch 130/187 with 50 sentences...\n",
      "Processing movies batch 131/187 with 50 sentences...\n",
      "Processing movies batch 132/187 with 50 sentences...\n",
      "Processing movies batch 133/187 with 50 sentences...\n",
      "Processing movies batch 134/187 with 50 sentences...\n",
      "Processing movies batch 135/187 with 50 sentences...\n",
      "Processing movies batch 136/187 with 50 sentences...\n",
      "Processing movies batch 137/187 with 50 sentences...\n",
      "Processing movies batch 138/187 with 50 sentences...\n",
      "Processing movies batch 139/187 with 50 sentences...\n",
      "Processing movies batch 140/187 with 50 sentences...\n",
      "Processing movies batch 141/187 with 50 sentences...\n",
      "Processing movies batch 142/187 with 50 sentences...\n",
      "Processing movies batch 143/187 with 50 sentences...\n",
      "Processing movies batch 144/187 with 50 sentences...\n",
      "Processing movies batch 145/187 with 50 sentences...\n",
      "Processing movies batch 146/187 with 50 sentences...\n",
      "Processing movies batch 147/187 with 50 sentences...\n",
      "Processing movies batch 148/187 with 50 sentences...\n",
      "Processing movies batch 149/187 with 50 sentences...\n",
      "Processing movies batch 150/187 with 50 sentences...\n",
      "Processing movies batch 151/187 with 50 sentences...\n",
      "Processing movies batch 152/187 with 50 sentences...\n",
      "Processing movies batch 153/187 with 50 sentences...\n",
      "Processing movies batch 154/187 with 50 sentences...\n",
      "Processing movies batch 155/187 with 50 sentences...\n",
      "Processing movies batch 156/187 with 50 sentences...\n",
      "Processing movies batch 157/187 with 50 sentences...\n",
      "Processing movies batch 158/187 with 50 sentences...\n",
      "Processing movies batch 159/187 with 50 sentences...\n",
      "Processing movies batch 160/187 with 50 sentences...\n",
      "Processing movies batch 161/187 with 50 sentences...\n",
      "Processing movies batch 162/187 with 50 sentences...\n",
      "Processing movies batch 163/187 with 50 sentences...\n",
      "Processing movies batch 164/187 with 50 sentences...\n",
      "Processing movies batch 165/187 with 50 sentences...\n",
      "Processing movies batch 166/187 with 50 sentences...\n",
      "Processing movies batch 167/187 with 50 sentences...\n",
      "Processing movies batch 168/187 with 50 sentences...\n",
      "Processing movies batch 169/187 with 50 sentences...\n",
      "Processing movies batch 170/187 with 50 sentences...\n",
      "Processing movies batch 171/187 with 50 sentences...\n",
      "Processing movies batch 172/187 with 50 sentences...\n",
      "Processing movies batch 173/187 with 50 sentences...\n",
      "Processing movies batch 174/187 with 50 sentences...\n",
      "Processing movies batch 175/187 with 50 sentences...\n",
      "Processing movies batch 176/187 with 50 sentences...\n",
      "Processing movies batch 177/187 with 50 sentences...\n",
      "Processing movies batch 178/187 with 50 sentences...\n",
      "Processing movies batch 179/187 with 50 sentences...\n",
      "Processing movies batch 180/187 with 50 sentences...\n",
      "Processing movies batch 181/187 with 50 sentences...\n",
      "Processing movies batch 182/187 with 50 sentences...\n",
      "Processing movies batch 183/187 with 50 sentences...\n",
      "Processing movies batch 184/187 with 50 sentences...\n",
      "Processing movies batch 185/187 with 50 sentences...\n",
      "Processing movies batch 186/187 with 50 sentences...\n",
      "Processing movies batch 187/187 with 5 sentences...\n",
      "Movies Embeddings processed successfully.\n"
     ]
    }
   ],
   "source": [
    "movies_embeddings_dict = {}  # Diccionario para almacenar {userId: embedding}\n",
    "num_batches = len(movies_batches_user_pref)  # cuantas batches tengo\n",
    "\n",
    "for i, batch in enumerate(movies_batches_user_pref, start=1):\n",
    "    batch_user_ids = batch['userId'].tolist()\n",
    "    batch_sentences = batch['movies_sentences_to_embed'].tolist()\n",
    "    \n",
    "    while True:\n",
    "        try: \n",
    "            print(f\"Processing movies batch {i}/{num_batches} with {len(batch)} sentences...\")\n",
    "            time.sleep(1)\n",
    "            embeddings = get_embeddings_from_api(batch_sentences)\n",
    "            \n",
    "            if embeddings:\n",
    "                movies_embeddings_dict.update({uid: emb for uid, emb in zip(batch_user_ids, embeddings)})\n",
    "                break\n",
    "        except ReadTimeout:\n",
    "            print(f'Timeout on batch {i}. Retrying...')\n",
    "            time.sleep(5)\n",
    "\n",
    "print(\"Movies Embeddings processed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing series batch 1/187 with 50 sentences...\n",
      "Processing series batch 2/187 with 50 sentences...\n",
      "Processing series batch 3/187 with 50 sentences...\n",
      "Processing series batch 4/187 with 50 sentences...\n",
      "Processing series batch 5/187 with 50 sentences...\n",
      "Processing series batch 6/187 with 50 sentences...\n",
      "Processing series batch 7/187 with 50 sentences...\n",
      "Processing series batch 8/187 with 50 sentences...\n",
      "Processing series batch 9/187 with 50 sentences...\n",
      "Processing series batch 10/187 with 50 sentences...\n",
      "Processing series batch 11/187 with 50 sentences...\n",
      "Processing series batch 12/187 with 50 sentences...\n",
      "Processing series batch 13/187 with 50 sentences...\n",
      "Processing series batch 14/187 with 50 sentences...\n",
      "Processing series batch 15/187 with 50 sentences...\n",
      "Processing series batch 16/187 with 50 sentences...\n",
      "Processing series batch 17/187 with 50 sentences...\n",
      "Processing series batch 18/187 with 50 sentences...\n",
      "Processing series batch 19/187 with 50 sentences...\n",
      "Processing series batch 20/187 with 50 sentences...\n",
      "Processing series batch 21/187 with 50 sentences...\n",
      "Processing series batch 22/187 with 50 sentences...\n",
      "Processing series batch 23/187 with 50 sentences...\n",
      "Processing series batch 24/187 with 50 sentences...\n",
      "Processing series batch 25/187 with 50 sentences...\n",
      "Processing series batch 26/187 with 50 sentences...\n",
      "Error 503: {\"error\":\"Model sentence-transformers/all-MiniLM-L6-v2 is currently loading\",\"estimated_time\":20.0}\n",
      "Processing series batch 26/187 with 50 sentences...\n",
      "Error 503: {\"error\":\"Model sentence-transformers/all-MiniLM-L6-v2 is currently loading\",\"estimated_time\":20.0}\n",
      "Processing series batch 26/187 with 50 sentences...\n",
      "Error 503: {\"error\":\"Model sentence-transformers/all-MiniLM-L6-v2 is currently loading\",\"estimated_time\":20.0}\n",
      "Processing series batch 26/187 with 50 sentences...\n",
      "Error 503: {\"error\":\"Model sentence-transformers/all-MiniLM-L6-v2 is currently loading\",\"estimated_time\":20.0}\n",
      "Processing series batch 26/187 with 50 sentences...\n",
      "Error 503: {\"error\":\"Model sentence-transformers/all-MiniLM-L6-v2 is currently loading\",\"estimated_time\":20.0}\n",
      "Processing series batch 26/187 with 50 sentences...\n",
      "Error 503: {\"error\":\"Model sentence-transformers/all-MiniLM-L6-v2 is currently loading\",\"estimated_time\":20.0}\n",
      "Processing series batch 26/187 with 50 sentences...\n",
      "Processing series batch 27/187 with 50 sentences...\n",
      "Processing series batch 28/187 with 50 sentences...\n",
      "Processing series batch 29/187 with 50 sentences...\n",
      "Processing series batch 30/187 with 50 sentences...\n",
      "Processing series batch 31/187 with 50 sentences...\n",
      "Processing series batch 32/187 with 50 sentences...\n",
      "Processing series batch 33/187 with 50 sentences...\n",
      "Processing series batch 34/187 with 50 sentences...\n",
      "Processing series batch 35/187 with 50 sentences...\n",
      "Processing series batch 36/187 with 50 sentences...\n",
      "Processing series batch 37/187 with 50 sentences...\n",
      "Processing series batch 38/187 with 50 sentences...\n",
      "Processing series batch 39/187 with 50 sentences...\n",
      "Processing series batch 40/187 with 50 sentences...\n",
      "Processing series batch 41/187 with 50 sentences...\n",
      "Processing series batch 42/187 with 50 sentences...\n",
      "Processing series batch 43/187 with 50 sentences...\n",
      "Processing series batch 44/187 with 50 sentences...\n",
      "Processing series batch 45/187 with 50 sentences...\n",
      "Processing series batch 46/187 with 50 sentences...\n",
      "Processing series batch 47/187 with 50 sentences...\n",
      "Processing series batch 48/187 with 50 sentences...\n",
      "Processing series batch 49/187 with 50 sentences...\n",
      "Processing series batch 50/187 with 50 sentences...\n",
      "Processing series batch 51/187 with 50 sentences...\n",
      "Processing series batch 52/187 with 50 sentences...\n",
      "Processing series batch 53/187 with 50 sentences...\n",
      "Processing series batch 54/187 with 50 sentences...\n",
      "Processing series batch 55/187 with 50 sentences...\n",
      "Processing series batch 56/187 with 50 sentences...\n",
      "Processing series batch 57/187 with 50 sentences...\n",
      "Processing series batch 58/187 with 50 sentences...\n",
      "Processing series batch 59/187 with 50 sentences...\n",
      "Processing series batch 60/187 with 50 sentences...\n",
      "Processing series batch 61/187 with 50 sentences...\n",
      "Processing series batch 62/187 with 50 sentences...\n",
      "Processing series batch 63/187 with 50 sentences...\n",
      "Processing series batch 64/187 with 50 sentences...\n",
      "Processing series batch 65/187 with 50 sentences...\n",
      "Processing series batch 66/187 with 50 sentences...\n",
      "Processing series batch 67/187 with 50 sentences...\n",
      "Processing series batch 68/187 with 50 sentences...\n",
      "Processing series batch 69/187 with 50 sentences...\n",
      "Processing series batch 70/187 with 50 sentences...\n",
      "Processing series batch 71/187 with 50 sentences...\n",
      "Processing series batch 72/187 with 50 sentences...\n",
      "Processing series batch 73/187 with 50 sentences...\n",
      "Processing series batch 74/187 with 50 sentences...\n",
      "Processing series batch 75/187 with 50 sentences...\n",
      "Processing series batch 76/187 with 50 sentences...\n",
      "Processing series batch 77/187 with 50 sentences...\n",
      "Processing series batch 78/187 with 50 sentences...\n",
      "Processing series batch 79/187 with 50 sentences...\n",
      "Processing series batch 80/187 with 50 sentences...\n",
      "Processing series batch 81/187 with 50 sentences...\n",
      "Processing series batch 82/187 with 50 sentences...\n",
      "Processing series batch 83/187 with 50 sentences...\n",
      "Processing series batch 84/187 with 50 sentences...\n",
      "Processing series batch 85/187 with 50 sentences...\n",
      "Processing series batch 86/187 with 50 sentences...\n",
      "Processing series batch 87/187 with 50 sentences...\n",
      "Processing series batch 88/187 with 50 sentences...\n",
      "Processing series batch 89/187 with 50 sentences...\n",
      "Processing series batch 90/187 with 50 sentences...\n",
      "Processing series batch 91/187 with 50 sentences...\n",
      "Processing series batch 92/187 with 50 sentences...\n",
      "Processing series batch 93/187 with 50 sentences...\n",
      "Processing series batch 94/187 with 50 sentences...\n",
      "Processing series batch 95/187 with 50 sentences...\n",
      "Processing series batch 96/187 with 50 sentences...\n",
      "Processing series batch 97/187 with 50 sentences...\n",
      "Processing series batch 98/187 with 50 sentences...\n",
      "Processing series batch 99/187 with 50 sentences...\n",
      "Processing series batch 100/187 with 50 sentences...\n",
      "Processing series batch 101/187 with 50 sentences...\n",
      "Processing series batch 102/187 with 50 sentences...\n",
      "Processing series batch 103/187 with 50 sentences...\n",
      "Processing series batch 104/187 with 50 sentences...\n",
      "Processing series batch 105/187 with 50 sentences...\n",
      "Processing series batch 106/187 with 50 sentences...\n",
      "Processing series batch 107/187 with 50 sentences...\n",
      "Processing series batch 108/187 with 50 sentences...\n",
      "Processing series batch 109/187 with 50 sentences...\n",
      "Processing series batch 110/187 with 50 sentences...\n",
      "Processing series batch 111/187 with 50 sentences...\n",
      "Processing series batch 112/187 with 50 sentences...\n",
      "Processing series batch 113/187 with 50 sentences...\n",
      "Processing series batch 114/187 with 50 sentences...\n",
      "Processing series batch 115/187 with 50 sentences...\n",
      "Processing series batch 116/187 with 50 sentences...\n",
      "Processing series batch 117/187 with 50 sentences...\n",
      "Processing series batch 118/187 with 50 sentences...\n",
      "Processing series batch 119/187 with 50 sentences...\n",
      "Processing series batch 120/187 with 50 sentences...\n",
      "Processing series batch 121/187 with 50 sentences...\n",
      "Processing series batch 122/187 with 50 sentences...\n",
      "Processing series batch 123/187 with 50 sentences...\n",
      "Processing series batch 124/187 with 50 sentences...\n",
      "Processing series batch 125/187 with 50 sentences...\n",
      "Processing series batch 126/187 with 50 sentences...\n",
      "Processing series batch 127/187 with 50 sentences...\n",
      "Processing series batch 128/187 with 50 sentences...\n",
      "Processing series batch 129/187 with 50 sentences...\n",
      "Processing series batch 130/187 with 50 sentences...\n",
      "Processing series batch 131/187 with 50 sentences...\n",
      "Processing series batch 132/187 with 50 sentences...\n",
      "Processing series batch 133/187 with 50 sentences...\n",
      "Processing series batch 134/187 with 50 sentences...\n",
      "Processing series batch 135/187 with 50 sentences...\n",
      "Processing series batch 136/187 with 50 sentences...\n",
      "Processing series batch 137/187 with 50 sentences...\n",
      "Processing series batch 138/187 with 50 sentences...\n",
      "Processing series batch 139/187 with 50 sentences...\n",
      "Processing series batch 140/187 with 50 sentences...\n",
      "Processing series batch 141/187 with 50 sentences...\n",
      "Processing series batch 142/187 with 50 sentences...\n",
      "Processing series batch 143/187 with 50 sentences...\n",
      "Processing series batch 144/187 with 50 sentences...\n",
      "Processing series batch 145/187 with 50 sentences...\n",
      "Processing series batch 146/187 with 50 sentences...\n",
      "Processing series batch 147/187 with 50 sentences...\n",
      "Processing series batch 148/187 with 50 sentences...\n",
      "Processing series batch 149/187 with 50 sentences...\n",
      "Processing series batch 150/187 with 50 sentences...\n",
      "Processing series batch 151/187 with 50 sentences...\n",
      "Processing series batch 152/187 with 50 sentences...\n",
      "Processing series batch 153/187 with 50 sentences...\n",
      "Processing series batch 154/187 with 50 sentences...\n",
      "Processing series batch 155/187 with 50 sentences...\n",
      "Processing series batch 156/187 with 50 sentences...\n",
      "Processing series batch 157/187 with 50 sentences...\n",
      "Processing series batch 158/187 with 50 sentences...\n",
      "Processing series batch 159/187 with 50 sentences...\n",
      "Processing series batch 160/187 with 50 sentences...\n",
      "Processing series batch 161/187 with 50 sentences...\n",
      "Processing series batch 162/187 with 50 sentences...\n",
      "Processing series batch 163/187 with 50 sentences...\n",
      "Processing series batch 164/187 with 50 sentences...\n",
      "Processing series batch 165/187 with 50 sentences...\n",
      "Processing series batch 166/187 with 50 sentences...\n",
      "Processing series batch 167/187 with 50 sentences...\n",
      "Processing series batch 168/187 with 50 sentences...\n",
      "Processing series batch 169/187 with 50 sentences...\n",
      "Processing series batch 170/187 with 50 sentences...\n",
      "Processing series batch 171/187 with 50 sentences...\n",
      "Processing series batch 172/187 with 50 sentences...\n",
      "Processing series batch 173/187 with 50 sentences...\n",
      "Processing series batch 174/187 with 50 sentences...\n",
      "Processing series batch 175/187 with 50 sentences...\n",
      "Processing series batch 176/187 with 50 sentences...\n",
      "Processing series batch 177/187 with 50 sentences...\n",
      "Processing series batch 178/187 with 50 sentences...\n",
      "Processing series batch 179/187 with 50 sentences...\n",
      "Processing series batch 180/187 with 50 sentences...\n",
      "Processing series batch 181/187 with 50 sentences...\n",
      "Processing series batch 182/187 with 50 sentences...\n",
      "Processing series batch 183/187 with 50 sentences...\n",
      "Processing series batch 184/187 with 50 sentences...\n",
      "Processing series batch 185/187 with 50 sentences...\n",
      "Processing series batch 186/187 with 50 sentences...\n",
      "Processing series batch 187/187 with 5 sentences...\n",
      "Series Embeddings processed successfully.\n"
     ]
    }
   ],
   "source": [
    "series_embeddings_dict = {}  # Diccionario para almacenar {userId: embedding}\n",
    "num_batches_series = len(series_batches_user_pref)  # Total de batches\n",
    "\n",
    "for i, batch in enumerate(series_batches_user_pref, start=1):\n",
    "    batch_user_ids = batch['userId'].tolist()\n",
    "    batch_sentences = batch['series_sentences_to_embed'].tolist()\n",
    "    \n",
    "    while True:  # Intentar hasta que se procese correctamente\n",
    "        try:\n",
    "            print(f\"Processing series batch {i}/{num_batches_series} with {len(batch)} sentences...\")\n",
    "            time.sleep(1)\n",
    "            embeddings = get_embeddings_from_api(batch_sentences)\n",
    "            \n",
    "            if embeddings:\n",
    "                series_embeddings_dict.update({uid: emb for uid, emb in zip(batch_user_ids, embeddings)})\n",
    "                break  # Salir del bucle si se proces√≥ correctamente\n",
    "        except ReadTimeout:\n",
    "            print(f\"Timeout on batch {i}. Retrying...\")\n",
    "            time.sleep(5)  # Esperar antes de reintentar\n",
    "\n",
    "print(\"Series Embeddings processed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 9084 embeddings to movies_embeddings.faiss with IDs in movies_user_ids.pkl\n",
      "Saved 9084 embeddings to series_embeddings.faiss with IDs in series_user_ids.pkl\n"
     ]
    }
   ],
   "source": [
    "# Celda en prueba, no confirmado su uso/funcionamiento... La idea es almacenar los embeddings para poder usarlos en pruebas locales sin estar consultando los embeddings\n",
    "# guardo embeddings en FAISS y los IDs en pickle\n",
    "def save_faiss_index(embeddings_dict, index_filename, ids_filename):\n",
    "    if not embeddings_dict:\n",
    "        print(f\"No data to save for {index_filename}\")\n",
    "        return\n",
    "    \n",
    "    user_ids = list(embeddings_dict.keys())\n",
    "    embeddings = np.array(list(embeddings_dict.values())).astype('float32')\n",
    "    \n",
    "    index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "    index.add(embeddings)\n",
    "    faiss.write_index(index, index_filename)\n",
    "    \n",
    "    with open(ids_filename, 'wb') as f:\n",
    "        pickle.dump(user_ids, f)\n",
    "    \n",
    "    print(f\"Saved {len(user_ids)} embeddings to {index_filename} with IDs in {ids_filename}\")\n",
    "\n",
    "save_faiss_index(movies_embeddings_dict, \"movies_embeddings.faiss\", \"movies_user_ids.pkl\")\n",
    "save_faiss_index(series_embeddings_dict, \"series_embeddings.faiss\", \"series_user_ids.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movies Similarities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir los embeddings a arrays de numpy para c√°lculos m√°s r√°pidos\n",
    "all_embeddings_from_filtered_data_array = np.array(list(all_embeddings_dict.values()))\n",
    "\n",
    "# Diccionarios para almacenar las recomendaciones\n",
    "movies_recommendations_dict = {}\n",
    "\n",
    "# Obtener recomendaciones para cada usuario en movies_embeddings_dict\n",
    "for user_id, user_embedding in movies_embeddings_dict.items():\n",
    "    user_embedding_array = np.array(user_embedding).reshape(1, -1)  # Asegurar la forma correcta\n",
    "    movies_content_similarities = cosine_similarity(user_embedding_array, all_embeddings_from_filtered_data_array).flatten()\n",
    "    \n",
    "    # Ordenar por similitud y seleccionar el top-10\n",
    "    movies_most_similar_indexes = movies_content_similarities.argsort()[::-1][:10]\n",
    "    \n",
    "    # Convertir los √≠ndices en IDs reales\n",
    "    movies_recommended_ids = [filtered_data.iloc[i]['ID'] for i in movies_most_similar_indexes]\n",
    "    \n",
    "    # Guardar en el diccionario\n",
    "    movies_recommendations_dict[user_id] = movies_recommended_ids\n",
    "\n",
    "\n",
    "# Mostrar ejemplos\n",
    "print(\"Ejemplo de recomendaciones para un usuario en Movies:\")\n",
    "example_user = list(movies_recommendations_dict.keys())[0]\n",
    "print(f\"Usuario: {example_user} - Recomendaciones: {movies_recommendations_dict[example_user]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tv Shows (Series) Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict para las recomendaciones\n",
    "series_recommendations_dict = {}\n",
    "\n",
    "number_target_series_recommend = 25\n",
    "\n",
    "# recommend's para cada usuario en series_embeddings_dict\n",
    "for user_id, user_embedding in series_embeddings_dict.items():\n",
    "    user_embedding_array = np.array(user_embedding).reshape(1, -1)  # Asegurar la forma correcta\n",
    "    series_content_similarities = cosine_similarity(user_embedding_array, all_embeddings_from_filtered_data_array).flatten()\n",
    "    \n",
    "    # orden por similitud y top-goal\n",
    "    series_most_similar_indexes = series_content_similarities.argsort()[::-1][:number_target_series_recommend]\n",
    "    \n",
    "    # paso los indices a IDs reales\n",
    "    series_recommended_ids = [filtered_data.iloc[i]['ID'] for i in series_most_similar_indexes]\n",
    "    \n",
    "    # Guardar en el diccionario\n",
    "    series_recommendations_dict[user_id] = series_recommended_ids\n",
    "\n",
    "# Mostrar ejemplos\n",
    "print(\"\\nEjemplo de recomendaciones para un usuario en Series:\")\n",
    "example_user = list(series_recommendations_dict.keys())[0]\n",
    "print(f\"Usuario: {example_user} - Recomendaciones: {series_recommendations_dict[example_user]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recomendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardo\n",
    "with open(\"movies_recommendations.json\", \"w\") as f:\n",
    "    json.dump(movies_recommendations_dict, f, indent=4)\n",
    "\n",
    "with open(\"series_recommendations.json\", \"w\") as f:\n",
    "    json.dump(series_recommendations_dict, f, indent=4)\n",
    "\n",
    "print(\"Recomendaciones guardadas en JSON\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
