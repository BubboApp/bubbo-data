{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import boto3\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from botocore.exceptions import ClientError\n",
    "from decimal import Decimal\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "#from torch.nn.functional import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uploading the environment variables and get the API key\n",
    "load_dotenv()\n",
    "HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "AWS_ACCESS_KEY = os.getenv(\"AWS_ACCESS_KEY\")\n",
    "AWS_SECRET_KEY = os.getenv(\"AWS_SECRET_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Preferences from DynamoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get the info from DynamoDB, user preferences\n",
    "CONFIG = {\n",
    "    'aws': {\n",
    "        'access_key': AWS_ACCESS_KEY,\n",
    "        'secret_key': AWS_SECRET_KEY,\n",
    "        'region': 'eu-west-3',\n",
    "        'table': 'User-7kkcm5dn2rb77hst5nh7gbdisa-staging'\n",
    "    },\n",
    "    'columns': ['userId', 'favoriteMoviesIds', 'favoriteGenresIds', 'favoriteSeriesIds'],\n",
    "}\n",
    "\n",
    "# conexion\n",
    "session = boto3.Session(\n",
    "    aws_access_key_id=CONFIG['aws']['access_key'],\n",
    "    aws_secret_access_key=CONFIG['aws']['secret_key'],\n",
    "    region_name=CONFIG['aws']['region']\n",
    ")\n",
    "\n",
    "table = session.resource('dynamodb').Table(CONFIG['aws']['table'])\n",
    "\n",
    "# Values to String\n",
    "def _process_value(value):\n",
    "    if isinstance(value, Decimal):\n",
    "        return str(int(value))\n",
    "    return str(value)\n",
    "\n",
    "# Retrive info from DynamoDB and gets a DataFrame\n",
    "def fetch_preferences():\n",
    "    try:\n",
    "        items = []\n",
    "        start_key = None\n",
    "\n",
    "        while True:\n",
    "            # scan with defined 'columns'  in previous 'CONFIG'\n",
    "            scan_params = {\n",
    "                'ProjectionExpression': ', '.join(CONFIG['columns'])\n",
    "            }\n",
    "            if start_key:\n",
    "                scan_params['ExclusiveStartKey'] = start_key\n",
    "\n",
    "            response = table.scan(**scan_params)\n",
    "            items.extend(response.get('Items', []))\n",
    "\n",
    "            # check for next pages\n",
    "            start_key = response.get('LastEvaluatedKey')\n",
    "            if not start_key:\n",
    "                break\n",
    "\n",
    "        # data extracted processing\n",
    "        processed_data = [{\n",
    "            'userId': _process_value(item.get('userId', '')),\n",
    "            'favoriteMoviesIds': ';'.join(map(_process_value, item.get('favoriteMoviesIds', []))),\n",
    "            'favoriteGenresIds': ';'.join(map(_process_value, item.get('favoriteGenresIds', []))),\n",
    "            'favoriteSeriesIds': ';'.join(map(_process_value, item.get('favoriteSeriesIds', [])))\n",
    "        } for item in items]\n",
    "\n",
    "        df = pd.DataFrame(processed_data)\n",
    "        return df\n",
    "\n",
    "    except ClientError as e:\n",
    "        print(f\"Error al conectar con DynamoDB: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# calling function to get the df\n",
    "user_pref = fetch_preferences()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates: 0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5133 entries, 0 to 13665\n",
      "Data columns (total 4 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   userId             5133 non-null   object\n",
      " 1   favoriteMoviesIds  5133 non-null   object\n",
      " 2   favoriteGenresIds  5133 non-null   object\n",
      " 3   favoriteSeriesIds  5133 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 200.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# limpio el dataframe dejando solo users con genero, movie_favs y tvshow_favs\n",
    "user_pref = user_pref[user_pref['userId'].str.len()==36]\n",
    "user_pref = user_pref.replace(\"\",pd.NA)\n",
    "user_pref = user_pref.dropna()\n",
    "user_pref.reset_index(inplace=True,drop=True)\n",
    "print(f'Duplicates: {user_pref.duplicated().sum()}')\n",
    "user_pref.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# en esta parte el codigo queda freezado mientras se trabaja en el contenido a obtener desde fire\n",
    "# ya que necesito ir a buscar los 'Genres', 'CleanTitle', y 'Synopsis' en la base de Fire Base para crear las sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Content Data from FireBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# URL de la base de datos\n",
    "url = \"https://bubbo-dfba0-default-rtdb.europe-west1.firebasedatabase.app/documents/Content_latest_ad_flixole_jsonl.json\"\n",
    "\n",
    "# Realizar la solicitud GET\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    # Convertir la respuesta en un diccionario\n",
    "    data = response.json()\n",
    "\n",
    "    # Obtener las primeras 10 claves\n",
    "    first_10 = list(data.items())[:10]  # Extrae los primeros 10 elementos como lista de pares clave-valor\n",
    "\n",
    "    # Mostrar los resultados\n",
    "    print(\"Primeros 10 registros:\")\n",
    "    for key, value in first_10:\n",
    "        print(f\"Clave: {key}, Valor: {value}\")\n",
    "else:\n",
    "    print(f\"Error al acceder a la base de datos: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From FireBase\n",
    "# filtered_data = pd.read_csv(r'../../../filtered_data.csv',sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences for vectorize from 'filtered_data' / AHORA DESDE FIREBASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the sentences to embed \n",
    " \n",
    "\n",
    "# Formatting as str-list to send to the model\n",
    "sentences_from_filtered_data = filtered_data.sentences_to_embed.dropna().astype(str).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the model on Hugging Face processes only requests that can be completed within 60 seconds, we need to divide the sentences into batches.\n",
    "def split_into_batches(sentences, batch_size):\n",
    "    return [sentences[i:i + batch_size] for i in range(0, len(sentences), batch_size)]\n",
    "\n",
    "# After trying with different values, we've reach the maximum batch size to get response succesfully\n",
    "batches = split_into_batches(sentences_from_filtered_data, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check key availability\n",
    "if HUGGINGFACE_API_KEY is None:\n",
    "    print(\"Error: No se encontr√≥ la clave de API de Hugging Face.\")\n",
    "else:\n",
    "    print(\"Clave de API cargada correctamente.\")\n",
    "\n",
    "# Model URL\n",
    "API_URL = \"https://api-inference.huggingface.co/pipeline/feature-extraction/sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# API header and key\n",
    "headers = {\"Authorization\": f\"Bearer {HUGGINGFACE_API_KEY}\"}  \n",
    "\n",
    "# Function to get embeddings from Hugging Face API\n",
    "def get_embeddings_from_api(sentences):\n",
    "    url = API_URL\n",
    "    payload = {\"inputs\": sentences}\n",
    "    \n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(f\"Error {response.status_code}: {response.text}\")\n",
    "        return None\n",
    "    \n",
    "\n",
    "# Sequence Batch Processing Process\n",
    "all_embeddings_from_filtered_data = []\n",
    "for batch in batches:\n",
    "    print(f\"Processing batch with {len(batch)} sentences...\")\n",
    "    embeddings = get_embeddings_from_api(batch)\n",
    "    if embeddings:\n",
    "        all_embeddings_from_filtered_data.extend(embeddings)\n",
    "\n",
    "print(\"Embeddings processed successfully:\")\n",
    "print(all_embeddings_from_filtered_data[:2])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentences for vectorize from 'user_preferences' / AHORA DESDE DYNAMODB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentences we want to be embedded from user_preferences\n",
    "user_pref['sentences_to_embed'] = (user_pref.TitulosPeliculas.fillna('') +\n",
    "                                   user_pref.TitulosSeries.fillna('') + \n",
    "                                   user_pref.GenerosFavoritos.fillna('') +\n",
    "                                   user_pref.DetallesPeliculas.fillna('') +\n",
    "                                   user_pref.DetallesSeries.fillna(''))\n",
    "\n",
    "# Formatting as str-list to send to the model\n",
    "sentences_from_user_pref = user_pref.sentences_to_embed.dropna().astype(str).tolist()\n",
    "\n",
    "# We split the sentences in batches as we did previously with filtered_data\n",
    "batches_user_pref = split_into_batches(sentences_from_user_pref, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequence Batch Processing Process\n",
    "all_embeddings_from_user_pref = []\n",
    "for batch in batches_user_pref:\n",
    "    print(f\"Processing batch with {len(batch)} sentences...\")\n",
    "    embeddings = get_embeddings_from_api(batch)\n",
    "    if embeddings:\n",
    "        all_embeddings_from_user_pref.extend(embeddings)\n",
    "\n",
    "print(\"Embeddings processed successfully:\")\n",
    "print(all_embeddings_from_user_pref[:2])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to convert the embeddings list to numpy arrays in order to calculate cosine similarities wiht sklearn\n",
    "all_embeddings_from_user_pref_array = np.array(all_embeddings_from_user_pref)\n",
    "all_embeddings_from_filtered_data_array = np.array(all_embeddings_from_filtered_data)\n",
    "\n",
    "user_for_example = 1\n",
    "\n",
    "# Taking first user as example to calculate the cosine_similarity\n",
    "user_embedding_example = all_embeddings_from_user_pref_array[user_for_example].reshape(1, -1)  # Asegurar forma correcta para cosine_similarity\n",
    "\n",
    "# To calculate similarity between the user example embeding and the whole content from filtered data\n",
    "content_similarities = cosine_similarity(user_embedding_example, all_embeddings_from_filtered_data_array).flatten()\n",
    "\n",
    "# Sort indexes by similarity\n",
    "most_similar_indexes = content_similarities.argsort()[::-1]\n",
    "\n",
    "# Top-10\n",
    "topten_most_similar_indexes = most_similar_indexes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To display the most similar indexes and their similarity scores\n",
    "print(\"Most similar indexes:\", most_similar_indexes[:10])\n",
    "print(\"Highest similarities:\", content_similarities[most_similar_indexes[:10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search most_similar_indexes, and preferences, and get recommendations\n",
    "user_id = user_pref.loc[user_for_example]['userId']\n",
    "movies_preferred = user_pref[user_pref['userId']==user_id]['TitulosPeliculas']\n",
    "series_preferred = user_pref[user_pref['userId']==user_id]['TitulosSeries']\n",
    "\n",
    "# Displaying Preferences & Recommendations\n",
    "print(f'''User {user_id} Preferences:\n",
    "      ''')\n",
    "print(f'''Movies preference:''')\n",
    "for movie in movies_preferred.iloc[0].split(';'):\n",
    "    print(f'      {movie.strip()}')\n",
    "\n",
    "print(f'''TV Shows preference:''')\n",
    "for series in series_preferred.iloc[0].split(';'):\n",
    "    print(f'      {series.strip()}')\n",
    "\n",
    "print(f'''Recomendations for user: {user_id}\n",
    "      ''')\n",
    "\n",
    "recomendations_user = filtered_data.loc[topten_most_similar_indexes]['CleanTitle']\n",
    "for recommendation in recomendations_user:\n",
    "    print(f'      {recommendation}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
